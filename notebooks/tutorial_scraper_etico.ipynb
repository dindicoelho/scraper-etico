{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ü§ñ Scraper√âtico - Tutorial Completo\n\n**Bem-vindo ao tutorial interativo do Scraper√âtico!** \n\nEsta biblioteca Python faz **web scraping √©tico** respeitando automaticamente robots.txt, rate limiting e exportando dados para CSV/JSON.\n\n## üéØ O que voc√™ vai aprender neste tutorial:\n\n- ‚úÖ **Como verificar** se uma URL pode ser acessada (robots.txt)\n- ‚úÖ **Como fazer scraping** de forma √©tica e segura\n- ‚úÖ **Como processar m√∫ltiplas URLs** em lote com paraleliza√ß√£o\n- ‚úÖ **Como exportar dados** para CSV e JSON automaticamente\n- ‚úÖ **Como analisar resultados** com estat√≠sticas detalhadas\n- ‚úÖ **Boas pr√°ticas** para scraping √©tico\n\n## ‚ö†Ô∏è IMPORTANTE: \nEste tutorial usa sites **de exemplo seguros** (example.com, python.org). \n\nPara usar em **produ√ß√£o com seus sites reais**, voc√™ precisar√° editar o arquivo:\nüìÑ **`config_producao.py`** (copie de `config_producao.example.py`)\n\n**Vamos come√ßar! üöÄ**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üì¶ Passo 1: Instala√ß√£o e Configura√ß√£o\n\n**Primeiro, vamos instalar as depend√™ncias e importar as bibliotecas necess√°rias.**\n\nExecute a c√©lula abaixo para garantir que tudo esteja instalado:\n\nüí° **Arquivo necess√°rio**: `requirements.txt` (j√° deve estar na pasta do projeto)"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "# Instalar depend√™ncias (caso n√£o estejam instaladas)\n",
    "!pip3 install requests\n",
    "# Opcionais: !pip3 install pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Pandas/Matplotlib n√£o instalado - algumas funcionalidades estar√£o limitadas\n",
      "   Para instalar: pip3 install pandas matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Importar as classes principais\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from scraper_etico import ScraperEtico\n",
    "from analyzer import RobotsAnalyzer\n",
    "from batch_processor import BatchProcessor\n",
    "\n",
    "# Imports opcionais - comentar se n√£o tiver instalado\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    PANDAS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Pandas/Matplotlib n√£o instalado - algumas funcionalidades estar√£o limitadas\")\n",
    "    print(\"   Para instalar: pip3 install pandas matplotlib\")\n",
    "    PANDAS_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üöÄ Passo 2: Criar seu Primeiro Scraper\n\n**Agora vamos criar uma inst√¢ncia do Scraper√âtico e configur√°-la adequadamente.**\n\n‚ö†Ô∏è **IMPORTANTE:** Sempre configure um user-agent descritivo com suas informa√ß√µes reais!\n\nüìÑ **Arquivos relacionados**: \n- `src/scraper_etico.py` (c√≥digo principal - n√£o edite)\n- `config_producao.py` (suas configura√ß√µes - edite este para produ√ß√£o)"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:09 - ScraperEtico - INFO - ScraperEtico initialized with user-agent: MeuBot/1.0 (exemplo.com/contato)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraper√âtico inicializado com sucesso!\n",
      "User-Agent: MeuBot/1.0 (exemplo.com/contato)\n",
      "Delay padr√£o: 1.0s\n"
     ]
    }
   ],
   "source": [
    "# Criar inst√¢ncia do scraper\n",
    "scraper = ScraperEtico(\n",
    "    user_agent=\"MeuBot/1.0 (exemplo.com/contato)\",\n",
    "    default_delay=1.0  # 1 segundo entre requests\n",
    ")\n",
    "\n",
    "print(\"Scraper√âtico inicializado com sucesso!\")\n",
    "print(f\"User-Agent: {scraper.user_agent}\")\n",
    "print(f\"Delay padr√£o: {scraper.default_delay}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testando: https://example.com/\n",
      "üìÑ Verificando robots.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:10 - ScraperEtico - INFO - Successfully fetched robots.txt from https://example.com/robots.txt\n",
      "2025-09-04 12:58:10 - ScraperEtico - INFO - Fetching URL: https://example.com/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Robots.txt permite acesso: True\n",
      "‚è±Ô∏è  Usando delay padr√£o: 1.0 segundos\n",
      "\n",
      "üì° Fazendo request √©tico...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:11 - ScraperEtico - INFO - Request completed: status=200, size=1256 bytes, time=0.71s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sucesso!\n",
      "   Status Code: 200\n",
      "   Tamanho da p√°gina: 1,256 caracteres\n",
      "   Content-Type: text/html\n",
      "   Preview: <!doctype html> <html> <head>     <title>Example Domain</title>      <meta charset=\"utf-8\" />     <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />     <meta name=\"viewport\" conten...\n"
     ]
    }
   ],
   "source": [
    "# üß™ Passo 3: Primeiro Teste - Verificar uma URL\n",
    "\n",
    "# Vamos testar com um site seguro e confi√°vel\n",
    "url_teste = \"https://example.com/\"\n",
    "\n",
    "print(f\"üîç Testando: {url_teste}\")\n",
    "print(f\"üìÑ Verificando robots.txt...\")\n",
    "\n",
    "# 1. Verificar se o site permite scraping\n",
    "pode_acessar = scraper.can_fetch(url_teste)\n",
    "print(f\"‚úÖ Robots.txt permite acesso: {pode_acessar}\")\n",
    "\n",
    "# 2. Verificar se h√° crawl-delay espec√≠fico\n",
    "delay = scraper.get_crawl_delay(url_teste)\n",
    "if delay:\n",
    "    print(f\"‚è±Ô∏è  Site especifica crawl-delay: {delay} segundos\")\n",
    "else:\n",
    "    print(f\"‚è±Ô∏è  Usando delay padr√£o: {scraper.default_delay} segundos\")\n",
    "\n",
    "# 3. Se permitido, fazer o request\n",
    "if pode_acessar:\n",
    "    print(f\"\\nüì° Fazendo request √©tico...\")\n",
    "    response = scraper.get(url_teste)\n",
    "    \n",
    "    if response:\n",
    "        print(f\"‚úÖ Sucesso!\")\n",
    "        print(f\"   Status Code: {response.status_code}\")\n",
    "        print(f\"   Tamanho da p√°gina: {len(response.text):,} caracteres\")\n",
    "        print(f\"   Content-Type: {response.headers.get('content-type', 'N/A')}\")\n",
    "        \n",
    "        # Mostrar in√≠cio do conte√∫do\n",
    "        preview = response.text[:200].replace('\\n', ' ')\n",
    "        print(f\"   Preview: {preview}...\")\n",
    "    else:\n",
    "        print(\"‚ùå Request falhou\")\n",
    "else:\n",
    "    print(\"‚ùå Site n√£o permite scraping - respeitando robots.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä An√°lise Avan√ßada de robots.txt\n",
    "\n",
    "O RobotsAnalyzer permite fazer an√°lises detalhadas de arquivos robots.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç An√°lise manual do robots.txt de https://www.python.org\n",
      "üìÑ Robots.txt encontrado: ‚úÖ\n",
      "ü§ñ User-agents definidos: 6\n",
      "üö´ Total de regras: 7\n",
      "üó∫Ô∏è  Sitemaps: 0\n",
      "\n",
      "üìù Primeiras 10 linhas:\n",
      "   # Directions for robots.  See this URL:\n",
      "   # http://www.robotstxt.org/robotstxt.html\n",
      "   # for a description of the file format.\n",
      "   User-agent: HTTrack\n",
      "   User-agent: puf\n",
      "   User-agent: MSIECrawler\n",
      "   Disallow: /\n",
      "   # The Krugle web crawler (though based on Nutch) is OK.\n"
     ]
    }
   ],
   "source": [
    "# Criar inst√¢ncia do analisador\n",
    "analyzer = RobotsAnalyzer()\n",
    "\n",
    "# Analisar robots.txt de um site conhecido\n",
    "site_teste = \"https://www.python.org\"\n",
    "\n",
    "# Primeiro, baixar o conte√∫do do robots.txt\n",
    "try:\n",
    "    import requests\n",
    "    response = requests.get(f\"{site_teste}/robots.txt\", timeout=10)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"üîç An√°lise manual do robots.txt de {site_teste}\")\n",
    "        \n",
    "        # An√°lise simples do conte√∫do\n",
    "        robots_content = response.text\n",
    "        lines = robots_content.split('\\n')\n",
    "        \n",
    "        # Contar elementos\n",
    "        user_agents = sum(1 for line in lines if line.lower().startswith('user-agent:'))\n",
    "        disallows = sum(1 for line in lines if line.lower().startswith('disallow:'))\n",
    "        allows = sum(1 for line in lines if line.lower().startswith('allow:'))\n",
    "        sitemaps = sum(1 for line in lines if line.lower().startswith('sitemap:'))\n",
    "        \n",
    "        print(f\"üìÑ Robots.txt encontrado: ‚úÖ\")\n",
    "        print(f\"ü§ñ User-agents definidos: {user_agents}\")\n",
    "        print(f\"üö´ Total de regras: {disallows + allows}\")\n",
    "        print(f\"üó∫Ô∏è  Sitemaps: {sitemaps}\")\n",
    "        \n",
    "        # Mostrar primeiras linhas\n",
    "        print(f\"\\nüìù Primeiras 10 linhas:\")\n",
    "        for i, line in enumerate(lines[:10]):\n",
    "            if line.strip():\n",
    "                print(f\"   {line}\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao analisar: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üó∫Ô∏è Sitemaps encontrados:\n",
      "\n",
      "ü§ñ Alguns User-agents:\n",
      "  - HTTrack\n",
      "  - puf\n",
      "  - MSIECrawler\n",
      "  - Krugle\n",
      "  - Nutch\n"
     ]
    }
   ],
   "source": [
    "# Mostrar exemplos de sitemaps encontrados (se houver)\n",
    "if 'robots_content' in locals():\n",
    "    print(\"\\nüó∫Ô∏è Sitemaps encontrados:\")\n",
    "    for line in robots_content.split('\\n'):\n",
    "        if line.lower().startswith('sitemap:'):\n",
    "            print(f\"  - {line.split(':', 1)[1].strip()}\")\n",
    "    \n",
    "    # Mostrar alguns user-agents\n",
    "    print(\"\\nü§ñ Alguns User-agents:\")\n",
    "    count = 0\n",
    "    for line in robots_content.split('\\n'):\n",
    "        if line.lower().startswith('user-agent:') and count < 5:\n",
    "            print(f\"  - {line.split(':', 1)[1].strip()}\")\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üîÑ Passo 4: Processamento em Lote (M√∫ltiplas URLs)\n\n**Agora vamos processar v√°rias URLs de uma s√≥ vez usando o BatchProcessor.**\n\nIsso √© √∫til quando voc√™ quer monitorar m√∫ltiplos sites automaticamente.\n\nüìÑ **Arquivos relacionados**:\n- `src/batch_processor.py` (c√≥digo do processamento - n√£o edite)\n- Para produ√ß√£o, edite a lista de sites em: `config_producao.py` ‚Üí `SITES_PRODUCAO`"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:11 - ScraperEtico - INFO - ScraperEtico initialized with user-agent: ScraperEtico/1.0 (Ethical Web Scraper)\n",
      "2025-09-04 12:58:11 - BatchProcessor - INFO - BatchProcessor initialized with state dir: batch_states\n",
      "2025-09-04 12:58:11 - ScraperEtico - INFO - ScraperEtico initialized with user-agent: Tutorial/1.0 (aprendendo-scraping-etico)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Processador em lote criado\n",
      "üîó URLs para processar: 5\n",
      "ü§ñ User-agent: Tutorial/1.0 (aprendendo-scraping-etico)\n",
      "‚è±Ô∏è  Delay padr√£o: 1.5s\n"
     ]
    }
   ],
   "source": [
    "# Lista de URLs para testar\n",
    "urls_teste = [\n",
    "    \"https://httpbin.org/get\",\n",
    "    \"https://www.python.org/about/\",\n",
    "    \"https://docs.python.org/3/\",\n",
    "    \"https://github.com/python\",\n",
    "    \"https://stackoverflow.com/questions\"\n",
    "]\n",
    "\n",
    "# Criar processador em lote - API correta (sem par√¢metros no construtor)\n",
    "batch_processor = BatchProcessor()\n",
    "\n",
    "# Configurar ScraperEtico com par√¢metros desejados\n",
    "batch_processor.scraper = ScraperEtico(\n",
    "    user_agent=\"Tutorial/1.0 (aprendendo-scraping-etico)\",\n",
    "    default_delay=1.5\n",
    ")\n",
    "\n",
    "print(f\"üì¶ Processador em lote criado\")\n",
    "print(f\"üîó URLs para processar: {len(urls_teste)}\")\n",
    "print(f\"ü§ñ User-agent: {batch_processor.scraper.user_agent}\")\n",
    "print(f\"‚è±Ô∏è  Delay padr√£o: {batch_processor.scraper.default_delay}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:11 - BatchProcessor - INFO - Starting batch job 'batch_20250904_125811' with 5 URLs, 2 workers, analyze_robots=True\n",
      "2025-09-04 12:58:11 - ScraperEtico - INFO - Successfully fetched robots.txt from https://www.python.org/robots.txt\n",
      "2025-09-04 12:58:11 - ScraperEtico - INFO - Fetching URL: https://www.python.org/about/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando processamento em lote...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:11 - ScraperEtico - INFO - Request completed: status=200, size=43804 bytes, time=0.09s\n",
      "2025-09-04 12:58:11 - BatchProcessor - INFO - Successfully processed: https://www.python.org/about/ (status: 200, size: 43804 bytes, time: 0.09s)\n",
      "2025-09-04 12:58:11 - RobotsAnalyzer - INFO - Fetching robots.txt from: https://www.python.org/robots.txt\n",
      "2025-09-04 12:58:11 - RobotsAnalyzer - INFO - Successfully fetched robots.txt (537 chars)\n",
      "2025-09-04 12:58:11 - RobotsAnalyzer - INFO - Parsed robots.txt: 6 user-agents, 0 sitemaps, 0 errors\n",
      "2025-09-04 12:58:11 - ScraperEtico - INFO - Successfully fetched robots.txt from https://docs.python.org/robots.txt\n",
      "2025-09-04 12:58:11 - ScraperEtico - INFO - Fetching URL: https://docs.python.org/3/\n",
      "2025-09-04 12:58:11 - ScraperEtico - INFO - Request completed: status=200, size=17874 bytes, time=0.09s\n",
      "2025-09-04 12:58:11 - BatchProcessor - INFO - Successfully processed: https://docs.python.org/3/ (status: 200, size: 17874 bytes, time: 0.09s)\n",
      "2025-09-04 12:58:11 - RobotsAnalyzer - INFO - Fetching robots.txt from: https://docs.python.org/robots.txt\n",
      "2025-09-04 12:58:12 - RobotsAnalyzer - INFO - Successfully fetched robots.txt (484 chars)\n",
      "2025-09-04 12:58:12 - RobotsAnalyzer - INFO - Parsed robots.txt: 1 user-agents, 1 sitemaps, 0 errors\n",
      "2025-09-04 12:58:12 - ScraperEtico - INFO - Successfully fetched robots.txt from https://github.com/robots.txt\n",
      "2025-09-04 12:58:12 - ScraperEtico - INFO - Fetching URL: https://github.com/python\n",
      "2025-09-04 12:58:12 - ScraperEtico - INFO - Request completed: status=200, size=297150 bytes, time=0.59s\n",
      "2025-09-04 12:58:13 - BatchProcessor - INFO - Successfully processed: https://github.com/python (status: 200, size: 297150 bytes, time: 0.60s)\n",
      "2025-09-04 12:58:13 - RobotsAnalyzer - INFO - Fetching robots.txt from: https://github.com/robots.txt\n",
      "2025-09-04 12:58:13 - RobotsAnalyzer - INFO - Successfully fetched robots.txt (1551 chars)\n",
      "2025-09-04 12:58:13 - RobotsAnalyzer - INFO - Parsed robots.txt: 2 user-agents, 0 sitemaps, 0 errors\n",
      "2025-09-04 12:58:13 - ScraperEtico - INFO - Successfully fetched robots.txt from https://stackoverflow.com/robots.txt\n",
      "2025-09-04 12:58:13 - ScraperEtico - INFO - Fetching URL: https://stackoverflow.com/questions\n",
      "2025-09-04 12:58:13 - ScraperEtico - INFO - Request completed: status=200, size=209936 bytes, time=0.41s\n",
      "2025-09-04 12:58:13 - BatchProcessor - INFO - Successfully processed: https://stackoverflow.com/questions (status: 200, size: 209936 bytes, time: 0.41s)\n",
      "2025-09-04 12:58:13 - RobotsAnalyzer - INFO - Fetching robots.txt from: https://stackoverflow.com/robots.txt\n",
      "2025-09-04 12:58:13 - RobotsAnalyzer - ERROR - HTTP error 418 fetching robots.txt from https://stackoverflow.com/robots.txt\n",
      "2025-09-04 12:58:18 - ScraperEtico - INFO - Successfully fetched robots.txt from https://httpbin.org/robots.txt\n",
      "2025-09-04 12:58:18 - ScraperEtico - INFO - Fetching URL: https://httpbin.org/get\n",
      "2025-09-04 12:58:19 - ScraperEtico - INFO - Request completed: status=200, size=325 bytes, time=1.19s\n",
      "2025-09-04 12:58:19 - BatchProcessor - INFO - Successfully processed: https://httpbin.org/get (status: 200, size: 325 bytes, time: 1.20s)\n",
      "2025-09-04 12:58:19 - RobotsAnalyzer - INFO - Fetching robots.txt from: https://httpbin.org/robots.txt\n",
      "2025-09-04 12:58:29 - RobotsAnalyzer - INFO - Successfully fetched robots.txt (30 chars)\n",
      "2025-09-04 12:58:29 - RobotsAnalyzer - INFO - Parsed robots.txt: 1 user-agents, 0 sitemaps, 0 errors\n",
      "2025-09-04 12:58:29 - BatchProcessor - INFO - Batch job 'batch_20250904_125811' completed: 5 successful, 0 failed, total time: 18.11s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Processamento conclu√≠do!\n",
      "üìä Estat√≠sticas:\n",
      "   Total: 5 URLs\n",
      "   Processadas: 5\n",
      "   Sucesso: 5\n",
      "   Falhas: 0\n",
      "   Taxa de sucesso: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Executar processamento em lote\n",
    "print(\"üöÄ Iniciando processamento em lote...\\n\")\n",
    "\n",
    "# Usar o m√©todo process_batch (n√£o processar_lote) com max_workers como par√¢metro\n",
    "job_state = batch_processor.process_batch(\n",
    "    urls_teste,\n",
    "    max_workers=2,  # max_workers vai aqui no m√©todo, n√£o no construtor\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚ú® Processamento conclu√≠do!\")\n",
    "print(f\"üìä Estat√≠sticas:\")\n",
    "print(f\"   Total: {job_state.total_urls} URLs\")\n",
    "print(f\"   Processadas: {job_state.processed_count}\")\n",
    "print(f\"   Sucesso: {len(job_state.completed_urls)}\")\n",
    "print(f\"   Falhas: {len(job_state.failed_urls)}\")\n",
    "print(f\"   Taxa de sucesso: {job_state.completion_percentage:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà An√°lise de Resultados\n",
    "\n",
    "Vamos analisar os resultados do processamento em lote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Resumo dos resultados:\n",
      "‚úÖ URLs com sucesso: 5\n",
      "‚ùå URLs com falha: 0\n",
      "ü§ñ URLs permitidas por robots.txt: 5\n",
      "\n",
      "üìã Detalhes:\n",
      "‚úÖü§ñ https://www.python.org/about/... - Status: 200\n",
      "‚úÖü§ñ https://docs.python.org/3/... - Status: 200\n",
      "‚úÖü§ñ https://github.com/python... - Status: 200\n",
      "‚úÖü§ñ https://stackoverflow.com/questions... - Status: 200\n",
      "‚úÖü§ñ https://httpbin.org/get... - Status: 200\n"
     ]
    }
   ],
   "source": [
    "# An√°lise dos resultados\n",
    "if PANDAS_AVAILABLE:\n",
    "    # Converter para DataFrame para an√°lise\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            'url': resultado.url,\n",
    "            'domain': resultado.domain,\n",
    "            'success': resultado.success,\n",
    "            'robots_allowed': resultado.robots_allowed,\n",
    "            'crawl_delay': resultado.crawl_delay,\n",
    "            'status_code': resultado.status_code,\n",
    "            'response_size': resultado.response_size,\n",
    "            'error_type': resultado.error_type\n",
    "        }\n",
    "        for resultado in job_state.results\n",
    "    ])\n",
    "    \n",
    "    print(\"üìä Resumo dos resultados:\")\n",
    "    print(f\"‚úÖ URLs com sucesso: {df['success'].sum()}\")\n",
    "    print(f\"‚ùå URLs com falha: {(~df['success']).sum()}\")\n",
    "    print(f\"ü§ñ URLs permitidas por robots.txt: {df['robots_allowed'].sum() if df['robots_allowed'].notna().any() else 'N/A'}\")\n",
    "    \n",
    "    # Mostrar tabela\n",
    "    print(\"\\nüìã Detalhes:\")\n",
    "    print(df[['url', 'success', 'robots_allowed', 'status_code']].head())\n",
    "else:\n",
    "    # An√°lise sem pandas\n",
    "    print(\"üìä Resumo dos resultados:\")\n",
    "    success_count = sum(1 for r in job_state.results if r.success)\n",
    "    total_count = len(job_state.results)\n",
    "    robots_allowed_count = sum(1 for r in job_state.results if r.robots_allowed)\n",
    "    \n",
    "    print(f\"‚úÖ URLs com sucesso: {success_count}\")\n",
    "    print(f\"‚ùå URLs com falha: {total_count - success_count}\")\n",
    "    print(f\"ü§ñ URLs permitidas por robots.txt: {robots_allowed_count}\")\n",
    "    \n",
    "    print(\"\\nüìã Detalhes:\")\n",
    "    for resultado in job_state.results[:5]:  # Mostrar primeiros 5\n",
    "        status = \"‚úÖ\" if resultado.success else \"‚ùå\"\n",
    "        robots_status = \"ü§ñ\" if resultado.robots_allowed else \"üö´\"\n",
    "        print(f\"{status}{robots_status} {resultado.url[:50]}... - Status: {resultado.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Visualiza√ß√µes n√£o dispon√≠veis - instale pandas e matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Criar visualiza√ß√£o (se pandas/matplotlib dispon√≠vel)\n",
    "if PANDAS_AVAILABLE:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Gr√°fico 1: Permitido vs Bloqueado\n",
    "    permitidos = df['allowed'].value_counts()\n",
    "    if len(permitidos) == 2:\n",
    "        labels = ['Bloqueado', 'Permitido'] if False in permitidos.index else ['Permitido']\n",
    "    else:\n",
    "        labels = ['Permitido' if permitidos.index[0] else 'Bloqueado']\n",
    "    \n",
    "    ax1.pie(permitidos.values, labels=labels, autopct='%1.1f%%', \n",
    "            colors=['#ff6b6b', '#4ecdc4'] if len(permitidos) == 2 else ['#4ecdc4'])\n",
    "    ax1.set_title('URLs: Permitidas vs Bloqueadas')\n",
    "    \n",
    "    # Gr√°fico 2: Sites com/sem robots.txt\n",
    "    robots = df['robots_found'].value_counts()\n",
    "    if len(robots) == 2:\n",
    "        labels = ['Sem robots.txt', 'Com robots.txt'] if False in robots.index else ['Com robots.txt']\n",
    "    else:\n",
    "        labels = ['Com robots.txt' if robots.index[0] else 'Sem robots.txt']\n",
    "        \n",
    "    ax2.pie(robots.values, labels=labels, autopct='%1.1f%%',\n",
    "            colors=['#ffa726', '#66bb6a'] if len(robots) == 2 else ['#66bb6a'])\n",
    "    ax2.set_title('Sites com robots.txt')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"üìä Visualiza√ß√µes n√£o dispon√≠veis - instale pandas e matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìä Passo 5: Exportar Resultados para CSV e JSON\n\n**Agora vamos exportar os dados para formatos que voc√™ pode usar no Excel, Python, ou outras ferramentas.**\n\nüìÑ **Arquivos gerados**:\n- `resultados_scraping_YYYYMMDD_HHMMSS.csv` (Excel/Google Sheets)\n- `resultados_scraping_YYYYMMDD_HHMMSS.json` (an√°lise program√°tica)\n\nüìÅ **Pasta de destino**: Arquivos ser√£o salvos na pasta atual do notebook"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:29 - BatchProcessor - INFO - Exported 5 results to CSV: resultados_scraping_20250904_125829.csv\n",
      "2025-09-04 12:58:29 - BatchProcessor - INFO - Exported 5 results to JSON: resultados_scraping_20250904_125829.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Resultados salvos em: resultados_scraping_20250904_125829.csv\n",
      "üìÑ Resultados salvos em: resultados_scraping_20250904_125829.json\n"
     ]
    }
   ],
   "source": [
    "# Exportar resultados\n",
    "from datetime import datetime\n",
    "\n",
    "# Nome do arquivo com timestamp\n",
    "nome_arquivo = f\"resultados_scraping_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "if PANDAS_AVAILABLE:\n",
    "    # Exportar com pandas\n",
    "    df.to_csv(f\"{nome_arquivo}.csv\", index=False)\n",
    "    print(f\"üìÑ Resultados salvos em: {nome_arquivo}.csv\")\n",
    "else:\n",
    "    # Exportar usando os m√©todos do BatchProcessor\n",
    "    batch_processor.export_to_csv(job_state, f\"{nome_arquivo}.csv\")\n",
    "    print(f\"üìÑ Resultados salvos em: {nome_arquivo}.csv\")\n",
    "\n",
    "# Exportar para JSON usando BatchProcessor\n",
    "batch_processor.export_to_json(job_state, f\"{nome_arquivo}.json\")\n",
    "print(f\"üìÑ Resultados salvos em: {nome_arquivo}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üõ°Ô∏è Passo 6: Princ√≠pios √âticos - O que Voc√™ DEVE Saber\n\n### ‚úÖ **O Scraper√âtico SEMPRE faz automaticamente:**\n\n- üîç **Verifica robots.txt** antes de cada acesso\n- ‚è±Ô∏è **Aplica delays** entre requests (nunca sobrecarrega)\n- ü§ñ **Identifica o bot** com user-agent claro\n- üìù **Gera logs completos** para auditoria\n- üö® **Para se bloqueado** (erro 429, robots.txt)\n\n### ‚ùå **O Scraper√âtico NUNCA faz:**\n\n- ‚ùå Ignora robots.txt ou termos de uso\n- ‚ùå Faz requests sem delay\n- ‚ùå Usa user-agents falsos de navegadores\n- ‚ùå Esconde a identidade do bot\n- ‚ùå Continua tentando quando bloqueado\n\n### üö® **Suas responsabilidades como usu√°rio:**\n\n1. **Configure user-agent** com SEU site e SEU email reais\n2. **Use delays adequados** (m√≠nimo 1s, recomendado 3-5s para sites gov)\n3. **Monitore logs** regularmente\n4. **Respeite termos de uso** dos sites\n5. **Tenha prop√≥sito leg√≠timo** para o scraping\n\n### üìû **Exemplo de User-Agent √âtico:**\n\n```python\n# ‚úÖ BOM - Identifica claramente quem voc√™ √©\nuser_agent = \"MeuProjeto/1.0 (+https://github.com/usuario/projeto; contato@email.com)\"\nuser_agent = \"PesquisaTCC/1.0 (+https://universidade.br/tcc; aluno@univ.br)\"  \nuser_agent = \"AnalisePublica/1.0 (+https://empresa.com/pesquisa; pesquisa@empresa.com)\"\n\n# ‚ùå RUIM - Gen√©rico demais\nuser_agent = \"MeuBot/1.0\"\nuser_agent = \"Python-requests/2.28\"  # Padr√£o do requests\n```\n\n### üìÑ **IMPORTANTE: Configure no arquivo `config_producao.py`**\nPara produ√ß√£o, copie `config_producao.example.py` ‚Üí `config_producao.py` e edite:\n- `USER_AGENT` - Com seus dados reais\n- `SITES_PRODUCAO` - Com seus sites para monitorar  \n- `DEFAULT_DELAY` - Conforme tipo de sites (gov = 5s+)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Configura√ß√µes Avan√ßadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:29 - ScraperEtico - INFO - ScraperEtico initialized with user-agent: MeuProjeto/2.0 (+http://meusite.com/sobre-bot)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Scraper personalizado configurado!\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de configura√ß√£o personalizada\n",
    "import logging\n",
    "\n",
    "scraper_personalizado = ScraperEtico(\n",
    "    user_agent=\"MeuProjeto/2.0 (+http://meusite.com/sobre-bot)\",\n",
    "    default_delay=2.0,  # Delay mais conservador\n",
    "    timeout=10.0,       # Timeout menor\n",
    "    log_level=logging.DEBUG  # Logs mais detalhados\n",
    ")\n",
    "\n",
    "# Acessar a sess√£o de requests para configurar headers\n",
    "import requests\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'User-Agent': scraper_personalizado.user_agent,\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',\n",
    "    'From': 'contato@meusite.com'  # Email para contato\n",
    "})\n",
    "\n",
    "# Usar a sess√£o customizada\n",
    "scraper_personalizado.session = session\n",
    "\n",
    "print(\"üîß Scraper personalizado configurado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Casos de Uso Pr√°ticos\n",
    "\n",
    "### 1. Verifica√ß√£o de Lista de Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:29 - ScraperEtico - DEBUG - Fetching robots.txt from: https://g1.globo.com/robots.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∞ Verificando sites de not√≠cias...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:29 - ScraperEtico - INFO - Successfully fetched robots.txt from https://g1.globo.com/robots.txt\n",
      "2025-09-04 12:58:29 - ScraperEtico - DEBUG - Robots.txt check for https://g1.globo.com/rss: allowed\n",
      "2025-09-04 12:58:29 - ScraperEtico - DEBUG - Fetching robots.txt from: https://folha.uol.com.br/robots.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Permitido - https://g1.globo.com/rss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:30 - ScraperEtico - INFO - Successfully fetched robots.txt from https://folha.uol.com.br/robots.txt\n",
      "2025-09-04 12:58:30 - ScraperEtico - DEBUG - Robots.txt check for https://folha.uol.com.br/rss: allowed\n",
      "2025-09-04 12:58:30 - ScraperEtico - DEBUG - Fetching robots.txt from: https://estadao.com.br/robots.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Permitido - https://folha.uol.com.br/rss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:30 - ScraperEtico - INFO - Successfully fetched robots.txt from https://estadao.com.br/robots.txt\n",
      "2025-09-04 12:58:30 - ScraperEtico - DEBUG - Robots.txt check for https://estadao.com.br/rss: allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Permitido - https://estadao.com.br/rss\n"
     ]
    }
   ],
   "source": [
    "# Lista de sites de not√≠cias para verificar\n",
    "sites_noticias = [\n",
    "    \"https://g1.globo.com/rss\",\n",
    "    \"https://folha.uol.com.br/rss\",\n",
    "    \"https://estadao.com.br/rss\",\n",
    "]\n",
    "\n",
    "print(\"üì∞ Verificando sites de not√≠cias...\")\n",
    "for site in sites_noticias:\n",
    "    try:\n",
    "        # API correta: can_fetch ao inv√©s de verificar_robots\n",
    "        resultado = scraper.can_fetch(site)\n",
    "        status = \"‚úÖ Permitido\" if resultado else \"‚ùå Bloqueado\"\n",
    "        print(f\"{status} - {site}\")\n",
    "        \n",
    "        # Verificar crawl-delay tamb√©m\n",
    "        delay = scraper.get_crawl_delay(site)\n",
    "        if delay:\n",
    "            print(f\"   ‚è±Ô∏è  Crawl-delay: {delay}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó Erro - {site}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. An√°lise de Diferentes User-Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:30 - ScraperEtico - INFO - ScraperEtico initialized with user-agent: *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Testando diferentes user-agents em https://example.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:31 - ScraperEtico - INFO - Successfully fetched robots.txt from https://example.com/robots.txt\n",
      "2025-09-04 12:58:31 - ScraperEtico - INFO - ScraperEtico initialized with user-agent: Googlebot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ *: Permitido\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:32 - ScraperEtico - INFO - Successfully fetched robots.txt from https://example.com/robots.txt\n",
      "2025-09-04 12:58:32 - ScraperEtico - INFO - ScraperEtico initialized with user-agent: Bingbot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Googlebot: Permitido\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:33 - ScraperEtico - INFO - Successfully fetched robots.txt from https://example.com/robots.txt\n",
      "2025-09-04 12:58:33 - ScraperEtico - INFO - ScraperEtico initialized with user-agent: MeuBot/1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bingbot: Permitido\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:33 - ScraperEtico - INFO - Successfully fetched robots.txt from https://example.com/robots.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MeuBot/1.0: Permitido\n"
     ]
    }
   ],
   "source": [
    "# Testar diferentes user-agents no mesmo site\n",
    "site_teste = \"https://example.com\"\n",
    "user_agents = [\n",
    "    \"*\",  # Todos os bots\n",
    "    \"Googlebot\",\n",
    "    \"Bingbot\", \n",
    "    \"MeuBot/1.0\"\n",
    "]\n",
    "\n",
    "print(f\"ü§ñ Testando diferentes user-agents em {site_teste}\")\n",
    "for ua in user_agents:\n",
    "    scraper_temp = ScraperEtico(user_agent=ua)\n",
    "    resultado = scraper_temp.can_fetch(site_teste)\n",
    "    status = \"‚úÖ\" if resultado else \"‚ùå\"\n",
    "    print(f\"{status} {ua}: {'Permitido' if resultado else 'Bloqueado'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üéì Parab√©ns! Voc√™ completou o tutorial\n\n### üéâ **O que voc√™ aprendeu:**\n\n- ‚úÖ Como fazer **scraping √©tico** respeitando robots.txt\n- ‚úÖ Como processar **m√∫ltiplos sites** em lote  \n- ‚úÖ Como **exportar dados** para CSV e JSON automaticamente\n- ‚úÖ Como configurar **delays e user-agents** adequados\n- ‚úÖ **Princ√≠pios √©ticos** fundamentais para web scraping\n\n### üöÄ **Pr√≥ximos passos para produ√ß√£o:**\n\n1. **üìÑ Configure suas credenciais reais no arquivo `config_producao.py`:**\n   ```bash\n   cp config_producao.example.py config_producao.py\n   nano config_producao.py  # OU use seu editor preferido\n   ```\n\n2. **üß™ Teste seus sites espec√≠ficos editando `teste_meus_sites.py`:**\n   ```bash\n   python3 teste_meus_sites.py\n   ```\n\n3. **üöÄ Execute scraping em produ√ß√£o com `rodar_producao.py`:**\n   ```bash\n   python3 rodar_producao.py\n   ```\n\n4. **üìä Monitore e analise resultados com `analisar_resultados.py`:**\n   ```bash\n   python3 analisar_resultados.py\n   open dados_producao/monitoramento_*.csv\n   ```\n\n### üìö **Arquivos importantes do projeto:**\n\n- **üìñ `README.md`** - Documenta√ß√£o completa\n- **üß™ `teste_producao.py`** - Testes antes de produ√ß√£o  \n- **üìä `analisar_resultados.py`** - An√°lise autom√°tica\n- **üîß `config_producao.py`** - Suas configura√ß√µes personalizadas\n- **üìù `config_producao.example.py`** - Template de configura√ß√£o\n- **üéØ `teste_meus_sites.py`** - Teste com seus sites espec√≠ficos\n- **‚ö° `rodar_producao.py`** - Script principal de produ√ß√£o\n- **üìã `checklist_producao.txt`** - Checklist antes de produ√ß√£o\n\n### üõ°Ô∏è **Lembre-se sempre:**\n\n> *\"Com grandes poderes v√™m grandes responsabilidades\"*\n\n- **Sempre respeite robots.txt** (nunca tente contornar)\n- **Use delays adequados** (m√≠nimo 3s para sites gov)  \n- **Identifique seu bot claramente** (user-agent com seus dados reais)\n- **Tenha prop√≥sito leg√≠timo** (pesquisa, monitoramento p√∫blico)\n- **Monitore logs regularmente** (pasta `logs/`)\n\n### üÜò **Precisa de ajuda?**\n\n- üìñ **Leia `README.md`** - Documenta√ß√£o completa com exemplos\n- üêõ **Problemas?** Abra uma issue no GitHub\n- üí¨ **D√∫vidas?** Use as GitHub Discussions\n\n**Agora voc√™ est√° pronto para fazer web scraping √©tico! ü§ñ‚ú®**\n\n### üí° **Dica final**: \nSempre comece testando com **poucos sites** (3-5) antes de escalar para centenas. O Scraper√âtico √© robusto, mas ser conservador √© sempre melhor!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}