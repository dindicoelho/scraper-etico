{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 🤖 EthicalScraper - Complete Tutorial\n\n**Welcome to the EthicalScraper interactive tutorial!** \n\nThis Python library performs **ethical web scraping** by automatically respecting robots.txt, rate limiting, and exporting data to CSV/JSON.\n\n## 🎯 What you'll learn in this tutorial:\n\n- ✅ **How to verify** if a URL can be accessed (robots.txt)\n- ✅ **How to perform scraping** ethically and safely\n- ✅ **How to process multiple URLs** in batches with parallelization\n- ✅ **How to export data** to CSV and JSON automatically\n- ✅ **How to analyze results** with detailed statistics\n- ✅ **Best practices** for ethical scraping\n\n## ⚠️ IMPORTANT: \nThis tutorial uses **safe example sites** (example.com, python.org). \n\nTo use in **production with your real sites**, you'll need to edit the file:\n📄 **`production_config.py`** (copy from `production_config.example.py`)\n\n**Let's get started! 🚀**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 📦 Step 1: Installation and Configuration\n\n**First, let's install the dependencies and import the necessary libraries.**\n\nRun the cell below to ensure everything is installed:\n\n💡 **Required file**: `requirements.txt` (should already be in the project folder)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies (if not already installed)\n!pip3 install requests\n# Optional: !pip3 install pandas matplotlib"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import main classes\nimport sys\nsys.path.append('../src')\n\nfrom scraper_etico import ScraperEtico\nfrom analyzer import RobotsAnalyzer\nfrom batch_processor import BatchProcessor\n\n# Optional imports - comment out if not installed\ntry:\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    PANDAS_AVAILABLE = True\nexcept ImportError:\n    print(\"⚠️ Pandas/Matplotlib not installed - some features will be limited\")\n    print(\"   To install: pip3 install pandas matplotlib\")\n    PANDAS_AVAILABLE = False"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 🚀 Step 2: Create Your First Scraper\n\n**Now let's create an EthicalScraper instance and configure it properly.**\n\n⚠️ **IMPORTANT:** Always configure a descriptive user-agent with your real information!\n\n📄 **Related files**: \n- `src/scraper_etico.py` (main code - don't edit)\n- `production_config.py` (your settings - edit this for production)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create scraper instance\nscraper = ScraperEtico(\n    user_agent=\"MyBot/1.0 (example.com/contact)\",\n    default_delay=1.0  # 1 second between requests\n)\n\nprint(\"EthicalScraper initialized successfully!\")\nprint(f\"User-Agent: {scraper.user_agent}\")\nprint(f\"Default delay: {scraper.default_delay}s\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 🧪 Step 3: First Test - Verify a URL\n\n# Let's test with a safe and reliable site\ntest_url = \"https://example.com/\"\n\nprint(f\"🔍 Testing: {test_url}\")\nprint(f\"📄 Checking robots.txt...\")\n\n# 1. Check if site allows scraping\ncan_access = scraper.can_fetch(test_url)\nprint(f\"✅ Robots.txt allows access: {can_access}\")\n\n# 2. Check for specific crawl-delay\ndelay = scraper.get_crawl_delay(test_url)\nif delay:\n    print(f\"⏱️  Site specifies crawl-delay: {delay} seconds\")\nelse:\n    print(f\"⏱️  Using default delay: {scraper.default_delay} seconds\")\n\n# 3. If allowed, make the request\nif can_access:\n    print(f\"\\n📡 Making ethical request...\")\n    response = scraper.get(test_url)\n    \n    if response:\n        print(f\"✅ Success!\")\n        print(f\"   Status Code: {response.status_code}\")\n        print(f\"   Page size: {len(response.text):,} characters\")\n        print(f\"   Content-Type: {response.headers.get('content-type', 'N/A')}\")\n        \n        # Show content preview\n        preview = response.text[:200].replace('\\n', ' ')\n        print(f\"   Preview: {preview}...\")\n    else:\n        print(\"❌ Request failed\")\nelse:\n    print(\"❌ Site doesn't allow scraping - respecting robots.txt\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 📊 Advanced robots.txt Analysis\n\nRobotsAnalyzer allows for detailed analysis of robots.txt files."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create analyzer instance\nanalyzer = RobotsAnalyzer()\n\n# Analyze robots.txt from a known site\ntest_site = \"https://www.python.org\"\n\n# First, download robots.txt content\ntry:\n    import requests\n    response = requests.get(f\"{test_site}/robots.txt\", timeout=10)\n    if response.status_code == 200:\n        print(f\"🔍 Manual analysis of robots.txt from {test_site}\")\n        \n        # Simple content analysis\n        robots_content = response.text\n        lines = robots_content.split('\\n')\n        \n        # Count elements\n        user_agents = sum(1 for line in lines if line.lower().startswith('user-agent:'))\n        disallows = sum(1 for line in lines if line.lower().startswith('disallow:'))\n        allows = sum(1 for line in lines if line.lower().startswith('allow:'))\n        sitemaps = sum(1 for line in lines if line.lower().startswith('sitemap:'))\n        \n        print(f\"📄 Robots.txt found: ✅\")\n        print(f\"🤖 User-agents defined: {user_agents}\")\n        print(f\"🚫 Total rules: {disallows + allows}\")\n        print(f\"🗺️  Sitemaps: {sitemaps}\")\n        \n        # Show first lines\n        print(f\"\\n📝 First 10 lines:\")\n        for i, line in enumerate(lines[:10]):\n            if line.strip():\n                print(f\"   {line}\")\n                \nexcept Exception as e:\n    print(f\"❌ Error analyzing: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🗺️ Sitemaps encontrados:\n",
      "\n",
      "🤖 Alguns User-agents:\n",
      "  - HTTrack\n",
      "  - puf\n",
      "  - MSIECrawler\n",
      "  - Krugle\n",
      "  - Nutch\n"
     ]
    }
   ],
   "source": [
    "# Mostrar exemplos de sitemaps encontrados (se houver)\n",
    "if 'robots_content' in locals():\n",
    "    print(\"\\n🗺️ Sitemaps encontrados:\")\n",
    "    for line in robots_content.split('\\n'):\n",
    "        if line.lower().startswith('sitemap:'):\n",
    "            print(f\"  - {line.split(':', 1)[1].strip()}\")\n",
    "    \n",
    "    # Mostrar alguns user-agents\n",
    "    print(\"\\n🤖 Alguns User-agents:\")\n",
    "    count = 0\n",
    "    for line in robots_content.split('\\n'):\n",
    "        if line.lower().startswith('user-agent:') and count < 5:\n",
    "            print(f\"  - {line.split(':', 1)[1].strip()}\")\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 🔄 Step 4: Batch Processing (Multiple URLs)\n\n**Now let's process multiple URLs at once using BatchProcessor.**\n\nThis is useful when you want to monitor multiple sites automatically.\n\n📄 **Related files**:\n- `src/batch_processor.py` (processing code - don't edit)\n- For production, edit the site list in: `production_config.py` → `PRODUCTION_SITES`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# List of URLs to test\ntest_urls = [\n    \"https://httpbin.org/get\",\n    \"https://www.python.org/about/\",\n    \"https://docs.python.org/3/\",\n    \"https://github.com/python\",\n    \"https://stackoverflow.com/questions\"\n]\n\n# Create batch processor - correct API (no parameters in constructor)\nbatch_processor = BatchProcessor()\n\n# Configure EthicalScraper with desired parameters\nbatch_processor.scraper = ScraperEtico(\n    user_agent=\"Tutorial/1.0 (learning-ethical-scraping)\",\n    default_delay=1.5\n)\n\nprint(f\"📦 Batch processor created\")\nprint(f\"🔗 URLs to process: {len(test_urls)}\")\nprint(f\"🤖 User-agent: {batch_processor.scraper.user_agent}\")\nprint(f\"⏱️  Default delay: {batch_processor.scraper.default_delay}s\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Execute batch processing\nprint(\"🚀 Starting batch processing...\\n\")\n\n# Use process_batch method (not processar_lote) with max_workers as parameter\njob_state = batch_processor.process_batch(\n    test_urls,\n    max_workers=2,  # max_workers goes here in the method, not in constructor\n    show_progress=True\n)\n\nprint(f\"\\n✨ Processing completed!\")\nprint(f\"📊 Statistics:\")\nprint(f\"   Total: {job_state.total_urls} URLs\")\nprint(f\"   Processed: {job_state.processed_count}\")\nprint(f\"   Success: {len(job_state.completed_urls)}\")\nprint(f\"   Failures: {len(job_state.failed_urls)}\")\nprint(f\"   Success rate: {job_state.completion_percentage:.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 📈 Results Analysis\n\nLet's analyze the results from batch processing."
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Resumo dos resultados:\n",
      "✅ URLs com sucesso: 5\n",
      "❌ URLs com falha: 0\n",
      "🤖 URLs permitidas por robots.txt: 5\n",
      "\n",
      "📋 Detalhes:\n",
      "✅🤖 https://www.python.org/about/... - Status: 200\n",
      "✅🤖 https://docs.python.org/3/... - Status: 200\n",
      "✅🤖 https://github.com/python... - Status: 200\n",
      "✅🤖 https://stackoverflow.com/questions... - Status: 200\n",
      "✅🤖 https://httpbin.org/get... - Status: 200\n"
     ]
    }
   ],
   "source": [
    "# Análise dos resultados\n",
    "if PANDAS_AVAILABLE:\n",
    "    # Converter para DataFrame para análise\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            'url': resultado.url,\n",
    "            'domain': resultado.domain,\n",
    "            'success': resultado.success,\n",
    "            'robots_allowed': resultado.robots_allowed,\n",
    "            'crawl_delay': resultado.crawl_delay,\n",
    "            'status_code': resultado.status_code,\n",
    "            'response_size': resultado.response_size,\n",
    "            'error_type': resultado.error_type\n",
    "        }\n",
    "        for resultado in job_state.results\n",
    "    ])\n",
    "    \n",
    "    print(\"📊 Resumo dos resultados:\")\n",
    "    print(f\"✅ URLs com sucesso: {df['success'].sum()}\")\n",
    "    print(f\"❌ URLs com falha: {(~df['success']).sum()}\")\n",
    "    print(f\"🤖 URLs permitidas por robots.txt: {df['robots_allowed'].sum() if df['robots_allowed'].notna().any() else 'N/A'}\")\n",
    "    \n",
    "    # Mostrar tabela\n",
    "    print(\"\\n📋 Detalhes:\")\n",
    "    print(df[['url', 'success', 'robots_allowed', 'status_code']].head())\n",
    "else:\n",
    "    # Análise sem pandas\n",
    "    print(\"📊 Resumo dos resultados:\")\n",
    "    success_count = sum(1 for r in job_state.results if r.success)\n",
    "    total_count = len(job_state.results)\n",
    "    robots_allowed_count = sum(1 for r in job_state.results if r.robots_allowed)\n",
    "    \n",
    "    print(f\"✅ URLs com sucesso: {success_count}\")\n",
    "    print(f\"❌ URLs com falha: {total_count - success_count}\")\n",
    "    print(f\"🤖 URLs permitidas por robots.txt: {robots_allowed_count}\")\n",
    "    \n",
    "    print(\"\\n📋 Detalhes:\")\n",
    "    for resultado in job_state.results[:5]:  # Mostrar primeiros 5\n",
    "        status = \"✅\" if resultado.success else \"❌\"\n",
    "        robots_status = \"🤖\" if resultado.robots_allowed else \"🚫\"\n",
    "        print(f\"{status}{robots_status} {resultado.url[:50]}... - Status: {resultado.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Visualizações não disponíveis - instale pandas e matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Criar visualização (se pandas/matplotlib disponível)\n",
    "if PANDAS_AVAILABLE:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Gráfico 1: Permitido vs Bloqueado\n",
    "    permitidos = df['allowed'].value_counts()\n",
    "    if len(permitidos) == 2:\n",
    "        labels = ['Bloqueado', 'Permitido'] if False in permitidos.index else ['Permitido']\n",
    "    else:\n",
    "        labels = ['Permitido' if permitidos.index[0] else 'Bloqueado']\n",
    "    \n",
    "    ax1.pie(permitidos.values, labels=labels, autopct='%1.1f%%', \n",
    "            colors=['#ff6b6b', '#4ecdc4'] if len(permitidos) == 2 else ['#4ecdc4'])\n",
    "    ax1.set_title('URLs: Permitidas vs Bloqueadas')\n",
    "    \n",
    "    # Gráfico 2: Sites com/sem robots.txt\n",
    "    robots = df['robots_found'].value_counts()\n",
    "    if len(robots) == 2:\n",
    "        labels = ['Sem robots.txt', 'Com robots.txt'] if False in robots.index else ['Com robots.txt']\n",
    "    else:\n",
    "        labels = ['Com robots.txt' if robots.index[0] else 'Sem robots.txt']\n",
    "        \n",
    "    ax2.pie(robots.values, labels=labels, autopct='%1.1f%%',\n",
    "            colors=['#ffa726', '#66bb6a'] if len(robots) == 2 else ['#66bb6a'])\n",
    "    ax2.set_title('Sites com robots.txt')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"📊 Visualizações não disponíveis - instale pandas e matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 📊 Step 5: Export Results to CSV and JSON\n\n**Now let's export the data to formats you can use in Excel, Python, or other tools.**\n\n📄 **Generated files**:\n- `scraping_results_YYYYMMDD_HHMMSS.csv` (Excel/Google Sheets)\n- `scraping_results_YYYYMMDD_HHMMSS.json` (programmatic analysis)\n\n📁 **Destination folder**: Files will be saved in the current notebook folder"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export results\nfrom datetime import datetime\n\n# File name with timestamp\nfile_name = f\"scraping_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n\nif PANDAS_AVAILABLE:\n    # Export with pandas\n    df.to_csv(f\"{file_name}.csv\", index=False)\n    print(f\"📄 Results saved to: {file_name}.csv\")\nelse:\n    # Export using BatchProcessor methods\n    batch_processor.export_to_csv(job_state, f\"{file_name}.csv\")\n    print(f\"📄 Results saved to: {file_name}.csv\")\n\n# Export to JSON using BatchProcessor\nbatch_processor.export_to_json(job_state, f\"{file_name}.json\")\nprint(f\"📄 Results saved to: {file_name}.json\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 🛡️ Passo 6: Princípios Éticos - O que Você DEVE Saber\n\n### ✅ **O ScraperÉtico SEMPRE faz automaticamente:**\n\n- 🔍 **Verifica robots.txt** antes de cada acesso\n- ⏱️ **Aplica delays** entre requests (nunca sobrecarrega)\n- 🤖 **Identifica o bot** com user-agent claro\n- 📝 **Gera logs completos** para auditoria\n- 🚨 **Para se bloqueado** (erro 429, robots.txt)\n\n### ❌ **O ScraperÉtico NUNCA faz:**\n\n- ❌ Ignora robots.txt ou termos de uso\n- ❌ Faz requests sem delay\n- ❌ Usa user-agents falsos de navegadores\n- ❌ Esconde a identidade do bot\n- ❌ Continua tentando quando bloqueado\n\n### 🚨 **Suas responsabilidades como usuário:**\n\n1. **Configure user-agent** com SEU site e SEU email reais\n2. **Use delays adequados** (mínimo 1s, recomendado 3-5s para sites gov)\n3. **Monitore logs** regularmente\n4. **Respeite termos de uso** dos sites\n5. **Tenha propósito legítimo** para o scraping\n\n### 📞 **Exemplo de User-Agent Ético:**\n\n```python\n# ✅ BOM - Identifica claramente quem você é\nuser_agent = \"MeuProjeto/1.0 (+https://github.com/usuario/projeto; contato@email.com)\"\nuser_agent = \"PesquisaTCC/1.0 (+https://universidade.br/tcc; aluno@univ.br)\"  \nuser_agent = \"AnalisePublica/1.0 (+https://empresa.com/pesquisa; pesquisa@empresa.com)\"\n\n# ❌ RUIM - Genérico demais\nuser_agent = \"MeuBot/1.0\"\nuser_agent = \"Python-requests/2.28\"  # Padrão do requests\n```\n\n### 📄 **IMPORTANTE: Configure no arquivo `config_producao.py`**\nPara produção, copie `config_producao.example.py` → `config_producao.py` e edite:\n- `USER_AGENT` - Com seus dados reais\n- `SITES_PRODUCAO` - Com seus sites para monitorar  \n- `DEFAULT_DELAY` - Conforme tipo de sites (gov = 5s+)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Configurações Avançadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:29 - ScraperEtico - INFO - ScraperEtico initialized with user-agent: MeuProjeto/2.0 (+http://meusite.com/sobre-bot)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Scraper personalizado configurado!\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de configuração personalizada\n",
    "import logging\n",
    "\n",
    "scraper_personalizado = ScraperEtico(\n",
    "    user_agent=\"MeuProjeto/2.0 (+http://meusite.com/sobre-bot)\",\n",
    "    default_delay=2.0,  # Delay mais conservador\n",
    "    timeout=10.0,       # Timeout menor\n",
    "    log_level=logging.DEBUG  # Logs mais detalhados\n",
    ")\n",
    "\n",
    "# Acessar a sessão de requests para configurar headers\n",
    "import requests\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'User-Agent': scraper_personalizado.user_agent,\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',\n",
    "    'From': 'contato@meusite.com'  # Email para contato\n",
    "})\n",
    "\n",
    "# Usar a sessão customizada\n",
    "scraper_personalizado.session = session\n",
    "\n",
    "print(\"🔧 Scraper personalizado configurado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Casos de Uso Práticos\n",
    "\n",
    "### 1. Verificação de Lista de Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:29 - ScraperEtico - DEBUG - Fetching robots.txt from: https://g1.globo.com/robots.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📰 Verificando sites de notícias...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:29 - ScraperEtico - INFO - Successfully fetched robots.txt from https://g1.globo.com/robots.txt\n",
      "2025-09-04 12:58:29 - ScraperEtico - DEBUG - Robots.txt check for https://g1.globo.com/rss: allowed\n",
      "2025-09-04 12:58:29 - ScraperEtico - DEBUG - Fetching robots.txt from: https://folha.uol.com.br/robots.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Permitido - https://g1.globo.com/rss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:30 - ScraperEtico - INFO - Successfully fetched robots.txt from https://folha.uol.com.br/robots.txt\n",
      "2025-09-04 12:58:30 - ScraperEtico - DEBUG - Robots.txt check for https://folha.uol.com.br/rss: allowed\n",
      "2025-09-04 12:58:30 - ScraperEtico - DEBUG - Fetching robots.txt from: https://estadao.com.br/robots.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Permitido - https://folha.uol.com.br/rss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:30 - ScraperEtico - INFO - Successfully fetched robots.txt from https://estadao.com.br/robots.txt\n",
      "2025-09-04 12:58:30 - ScraperEtico - DEBUG - Robots.txt check for https://estadao.com.br/rss: allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Permitido - https://estadao.com.br/rss\n"
     ]
    }
   ],
   "source": [
    "# Lista de sites de notícias para verificar\n",
    "sites_noticias = [\n",
    "    \"https://g1.globo.com/rss\",\n",
    "    \"https://folha.uol.com.br/rss\",\n",
    "    \"https://estadao.com.br/rss\",\n",
    "]\n",
    "\n",
    "print(\"📰 Verificando sites de notícias...\")\n",
    "for site in sites_noticias:\n",
    "    try:\n",
    "        # API correta: can_fetch ao invés de verificar_robots\n",
    "        resultado = scraper.can_fetch(site)\n",
    "        status = \"✅ Permitido\" if resultado else \"❌ Bloqueado\"\n",
    "        print(f\"{status} - {site}\")\n",
    "        \n",
    "        # Verificar crawl-delay também\n",
    "        delay = scraper.get_crawl_delay(site)\n",
    "        if delay:\n",
    "            print(f\"   ⏱️  Crawl-delay: {delay}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"❗ Erro - {site}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Análise de Diferentes User-Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:30 - ScraperEtico - INFO - ScraperEtico initialized with user-agent: *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Testando diferentes user-agents em https://example.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:31 - ScraperEtico - INFO - Successfully fetched robots.txt from https://example.com/robots.txt\n",
      "2025-09-04 12:58:31 - ScraperEtico - INFO - ScraperEtico initialized with user-agent: Googlebot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ *: Permitido\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:32 - ScraperEtico - INFO - Successfully fetched robots.txt from https://example.com/robots.txt\n",
      "2025-09-04 12:58:32 - ScraperEtico - INFO - ScraperEtico initialized with user-agent: Bingbot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Googlebot: Permitido\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:33 - ScraperEtico - INFO - Successfully fetched robots.txt from https://example.com/robots.txt\n",
      "2025-09-04 12:58:33 - ScraperEtico - INFO - ScraperEtico initialized with user-agent: MeuBot/1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Bingbot: Permitido\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:33 - ScraperEtico - INFO - Successfully fetched robots.txt from https://example.com/robots.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MeuBot/1.0: Permitido\n"
     ]
    }
   ],
   "source": [
    "# Testar diferentes user-agents no mesmo site\n",
    "site_teste = \"https://example.com\"\n",
    "user_agents = [\n",
    "    \"*\",  # Todos os bots\n",
    "    \"Googlebot\",\n",
    "    \"Bingbot\", \n",
    "    \"MeuBot/1.0\"\n",
    "]\n",
    "\n",
    "print(f\"🤖 Testando diferentes user-agents em {site_teste}\")\n",
    "for ua in user_agents:\n",
    "    scraper_temp = ScraperEtico(user_agent=ua)\n",
    "    resultado = scraper_temp.can_fetch(site_teste)\n",
    "    status = \"✅\" if resultado else \"❌\"\n",
    "    print(f\"{status} {ua}: {'Permitido' if resultado else 'Bloqueado'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 🎓 Congratulations! You completed the tutorial\n\n### 🎉 **What you learned:**\n\n- ✅ How to perform **ethical scraping** respecting robots.txt\n- ✅ How to process **multiple sites** in batches  \n- ✅ How to **export data** to CSV and JSON automatically\n- ✅ How to configure **delays and user-agents** properly\n- ✅ **Ethical principles** fundamental for web scraping\n\n### 🚀 **Next steps for production:**\n\n1. **📄 Configure your real credentials in `production_config.py`:**\n   ```bash\n   cp production_config.example.py production_config.py\n   nano production_config.py  # OR use your preferred editor\n   ```\n\n2. **🧪 Test your specific sites by editing `tests/examples/my_monitoring.py`:**\n   ```bash\n   python3 tests/examples/my_monitoring.py\n   ```\n\n3. **🚀 Execute production scraping with `run_production.py`:**\n   ```bash\n   python3 run_production.py\n   ```\n\n4. **📊 Monitor and analyze results with `analyze_results.py`:**\n   ```bash\n   python3 analyze_results.py\n   open production_data/monitoring_*.csv\n   ```\n\n### 📚 **Important project files:**\n\n- **📖 `README.md`** - Complete documentation\n- **🧪 `tests/production_test.py`** - Tests before production  \n- **📊 `analyze_results.py`** - Automatic analysis\n- **🔧 `production_config.py`** - Your custom settings\n- **📝 `production_config.example.py`** - Configuration template\n- **🎯 `tests/examples/my_monitoring.py`** - Test with your specific sites\n- **⚡ `run_production.py`** - Main production script\n- **📋 `production_checklist.txt`** - Checklist before production\n\n### 🛡️ **Always remember:**\n\n> *\"With great power comes great responsibility\"*\n\n- **Always respect robots.txt** (never try to circumvent)\n- **Use adequate delays** (minimum 3s for gov sites)  \n- **Identify your bot clearly** (user-agent with your real data)\n- **Have legitimate purpose** (research, public monitoring)\n- **Monitor logs regularly** (`logs/` folder)\n\n### 🆘 **Need help?**\n\n- 📖 **Read `README.md`** - Complete documentation with examples\n- 🐛 **Problems?** Open an issue on GitHub\n- 💬 **Questions?** Use GitHub Discussions\n\n**Now you're ready to perform ethical web scraping! 🤖✨**\n\n### 💡 **Final tip**: \nAlways start testing with **few sites** (3-5) before scaling to hundreds. EthicalScraper is robust, but being conservative is always better!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}