{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ¤– EthicalScraper - Complete Tutorial\n\n**Welcome to the EthicalScraper interactive tutorial!** \n\nThis Python library performs **ethical web scraping** by automatically respecting robots.txt, rate limiting, and exporting data to CSV/JSON.\n\n## ğŸ¯ What you'll learn in this tutorial:\n\n- âœ… **How to verify** if a URL can be accessed (robots.txt)\n- âœ… **How to perform scraping** ethically and safely\n- âœ… **How to process multiple URLs** in batches with parallelization\n- âœ… **How to export data** to CSV and JSON automatically\n- âœ… **How to analyze results** with detailed statistics\n- âœ… **Best practices** for ethical scraping\n\n## âš ï¸ IMPORTANT: \nThis tutorial uses **safe example sites** (example.com, python.org). \n\nTo use in **production with your real sites**, you'll need to edit the file:\nğŸ“„ **`production_config.py`** (copy from `production_config.example.py`)\n\n**Let's get started! ğŸš€**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“¦ Step 1: Installation and Configuration\n\n**First, let's install the dependencies and import the necessary libraries.**\n\nRun the cell below to ensure everything is installed:\n\nğŸ’¡ **Required file**: `requirements.txt` (should already be in the project folder)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies (if not already installed)\n!pip3 install requests\n# Optional: !pip3 install pandas matplotlib"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import main classes\nimport sys\nsys.path.append('../src')\n\nfrom scraper_etico import ScraperEtico\nfrom analyzer import RobotsAnalyzer\nfrom batch_processor import BatchProcessor\n\n# Optional imports - comment out if not installed\ntry:\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    PANDAS_AVAILABLE = True\nexcept ImportError:\n    print(\"âš ï¸ Pandas/Matplotlib not installed - some features will be limited\")\n    print(\"   To install: pip3 install pandas matplotlib\")\n    PANDAS_AVAILABLE = False"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸš€ Step 2: Create Your First Scraper\n\n**Now let's create an EthicalScraper instance and configure it properly.**\n\nâš ï¸ **IMPORTANT:** Always configure a descriptive user-agent with your real information!\n\nğŸ“„ **Related files**: \n- `src/scraper_etico.py` (main code - don't edit)\n- `production_config.py` (your settings - edit this for production)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create scraper instance\nscraper = ScraperEtico(\n    user_agent=\"MyBot/1.0 (example.com/contact)\",\n    default_delay=1.0  # 1 second between requests\n)\n\nprint(\"EthicalScraper initialized successfully!\")\nprint(f\"User-Agent: {scraper.user_agent}\")\nprint(f\"Default delay: {scraper.default_delay}s\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ§ª Step 3: First Test - Verify a URL\n\n# Let's test with a safe and reliable site\ntest_url = \"https://example.com/\"\n\nprint(f\"ğŸ” Testing: {test_url}\")\nprint(f\"ğŸ“„ Checking robots.txt...\")\n\n# 1. Check if site allows scraping\ncan_access = scraper.can_fetch(test_url)\nprint(f\"âœ… Robots.txt allows access: {can_access}\")\n\n# 2. Check for specific crawl-delay\ndelay = scraper.get_crawl_delay(test_url)\nif delay:\n    print(f\"â±ï¸  Site specifies crawl-delay: {delay} seconds\")\nelse:\n    print(f\"â±ï¸  Using default delay: {scraper.default_delay} seconds\")\n\n# 3. If allowed, make the request\nif can_access:\n    print(f\"\\nğŸ“¡ Making ethical request...\")\n    response = scraper.get(test_url)\n    \n    if response:\n        print(f\"âœ… Success!\")\n        print(f\"   Status Code: {response.status_code}\")\n        print(f\"   Page size: {len(response.text):,} characters\")\n        print(f\"   Content-Type: {response.headers.get('content-type', 'N/A')}\")\n        \n        # Show content preview\n        preview = response.text[:200].replace('\\n', ' ')\n        print(f\"   Preview: {preview}...\")\n    else:\n        print(\"âŒ Request failed\")\nelse:\n    print(\"âŒ Site doesn't allow scraping - respecting robots.txt\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“Š Advanced robots.txt Analysis\n\nRobotsAnalyzer allows for detailed analysis of robots.txt files."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create analyzer instance\nanalyzer = RobotsAnalyzer()\n\n# Analyze robots.txt from a known site\ntest_site = \"https://www.python.org\"\n\n# First, download robots.txt content\ntry:\n    import requests\n    response = requests.get(f\"{test_site}/robots.txt\", timeout=10)\n    if response.status_code == 200:\n        print(f\"ğŸ” Manual analysis of robots.txt from {test_site}\")\n        \n        # Simple content analysis\n        robots_content = response.text\n        lines = robots_content.split('\\n')\n        \n        # Count elements\n        user_agents = sum(1 for line in lines if line.lower().startswith('user-agent:'))\n        disallows = sum(1 for line in lines if line.lower().startswith('disallow:'))\n        allows = sum(1 for line in lines if line.lower().startswith('allow:'))\n        sitemaps = sum(1 for line in lines if line.lower().startswith('sitemap:'))\n        \n        print(f\"ğŸ“„ Robots.txt found: âœ…\")\n        print(f\"ğŸ¤– User-agents defined: {user_agents}\")\n        print(f\"ğŸš« Total rules: {disallows + allows}\")\n        print(f\"ğŸ—ºï¸  Sitemaps: {sitemaps}\")\n        \n        # Show first lines\n        print(f\"\\nğŸ“ First 10 lines:\")\n        for i, line in enumerate(lines[:10]):\n            if line.strip():\n                print(f\"   {line}\")\n                \nexcept Exception as e:\n    print(f\"âŒ Error analyzing: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ—ºï¸ Sitemaps encontrados:\n",
      "\n",
      "ğŸ¤– Alguns User-agents:\n",
      "  - HTTrack\n",
      "  - puf\n",
      "  - MSIECrawler\n",
      "  - Krugle\n",
      "  - Nutch\n"
     ]
    }
   ],
   "source": [
    "# Mostrar exemplos de sitemaps encontrados (se houver)\n",
    "if 'robots_content' in locals():\n",
    "    print(\"\\nğŸ—ºï¸ Sitemaps encontrados:\")\n",
    "    for line in robots_content.split('\\n'):\n",
    "        if line.lower().startswith('sitemap:'):\n",
    "            print(f\"  - {line.split(':', 1)[1].strip()}\")\n",
    "    \n",
    "    # Mostrar alguns user-agents\n",
    "    print(\"\\nğŸ¤– Alguns User-agents:\")\n",
    "    count = 0\n",
    "    for line in robots_content.split('\\n'):\n",
    "        if line.lower().startswith('user-agent:') and count < 5:\n",
    "            print(f\"  - {line.split(':', 1)[1].strip()}\")\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ”„ Step 4: Batch Processing (Multiple URLs)\n\n**Now let's process multiple URLs at once using BatchProcessor.**\n\nThis is useful when you want to monitor multiple sites automatically.\n\nğŸ“„ **Related files**:\n- `src/batch_processor.py` (processing code - don't edit)\n- For production, edit the site list in: `production_config.py` â†’ `PRODUCTION_SITES`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# List of URLs to test\ntest_urls = [\n    \"https://httpbin.org/get\",\n    \"https://www.python.org/about/\",\n    \"https://docs.python.org/3/\",\n    \"https://github.com/python\",\n    \"https://stackoverflow.com/questions\"\n]\n\n# Create batch processor - correct API (no parameters in constructor)\nbatch_processor = BatchProcessor()\n\n# Configure EthicalScraper with desired parameters\nbatch_processor.scraper = ScraperEtico(\n    user_agent=\"Tutorial/1.0 (learning-ethical-scraping)\",\n    default_delay=1.5\n)\n\nprint(f\"ğŸ“¦ Batch processor created\")\nprint(f\"ğŸ”— URLs to process: {len(test_urls)}\")\nprint(f\"ğŸ¤– User-agent: {batch_processor.scraper.user_agent}\")\nprint(f\"â±ï¸  Default delay: {batch_processor.scraper.default_delay}s\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Execute batch processing\nprint(\"ğŸš€ Starting batch processing...\\n\")\n\n# Use process_batch method (not processar_lote) with max_workers as parameter\njob_state = batch_processor.process_batch(\n    test_urls,\n    max_workers=2,  # max_workers goes here in the method, not in constructor\n    show_progress=True\n)\n\nprint(f\"\\nâœ¨ Processing completed!\")\nprint(f\"ğŸ“Š Statistics:\")\nprint(f\"   Total: {job_state.total_urls} URLs\")\nprint(f\"   Processed: {job_state.processed_count}\")\nprint(f\"   Success: {len(job_state.completed_urls)}\")\nprint(f\"   Failures: {len(job_state.failed_urls)}\")\nprint(f\"   Success rate: {job_state.completion_percentage:.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“ˆ Results Analysis\n\nLet's analyze the results from batch processing."
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Resumo dos resultados:\n",
      "âœ… URLs com sucesso: 5\n",
      "âŒ URLs com falha: 0\n",
      "ğŸ¤– URLs permitidas por robots.txt: 5\n",
      "\n",
      "ğŸ“‹ Detalhes:\n",
      "âœ…ğŸ¤– https://www.python.org/about/... - Status: 200\n",
      "âœ…ğŸ¤– https://docs.python.org/3/... - Status: 200\n",
      "âœ…ğŸ¤– https://github.com/python... - Status: 200\n",
      "âœ…ğŸ¤– https://stackoverflow.com/questions... - Status: 200\n",
      "âœ…ğŸ¤– https://httpbin.org/get... - Status: 200\n"
     ]
    }
   ],
   "source": [
    "# AnÃ¡lise dos resultados\n",
    "if PANDAS_AVAILABLE:\n",
    "    # Converter para DataFrame para anÃ¡lise\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            'url': resultado.url,\n",
    "            'domain': resultado.domain,\n",
    "            'success': resultado.success,\n",
    "            'robots_allowed': resultado.robots_allowed,\n",
    "            'crawl_delay': resultado.crawl_delay,\n",
    "            'status_code': resultado.status_code,\n",
    "            'response_size': resultado.response_size,\n",
    "            'error_type': resultado.error_type\n",
    "        }\n",
    "        for resultado in job_state.results\n",
    "    ])\n",
    "    \n",
    "    print(\"ğŸ“Š Resumo dos resultados:\")\n",
    "    print(f\"âœ… URLs com sucesso: {df['success'].sum()}\")\n",
    "    print(f\"âŒ URLs com falha: {(~df['success']).sum()}\")\n",
    "    print(f\"ğŸ¤– URLs permitidas por robots.txt: {df['robots_allowed'].sum() if df['robots_allowed'].notna().any() else 'N/A'}\")\n",
    "    \n",
    "    # Mostrar tabela\n",
    "    print(\"\\nğŸ“‹ Detalhes:\")\n",
    "    print(df[['url', 'success', 'robots_allowed', 'status_code']].head())\n",
    "else:\n",
    "    # AnÃ¡lise sem pandas\n",
    "    print(\"ğŸ“Š Resumo dos resultados:\")\n",
    "    success_count = sum(1 for r in job_state.results if r.success)\n",
    "    total_count = len(job_state.results)\n",
    "    robots_allowed_count = sum(1 for r in job_state.results if r.robots_allowed)\n",
    "    \n",
    "    print(f\"âœ… URLs com sucesso: {success_count}\")\n",
    "    print(f\"âŒ URLs com falha: {total_count - success_count}\")\n",
    "    print(f\"ğŸ¤– URLs permitidas por robots.txt: {robots_allowed_count}\")\n",
    "    \n",
    "    print(\"\\nğŸ“‹ Detalhes:\")\n",
    "    for resultado in job_state.results[:5]:  # Mostrar primeiros 5\n",
    "        status = \"âœ…\" if resultado.success else \"âŒ\"\n",
    "        robots_status = \"ğŸ¤–\" if resultado.robots_allowed else \"ğŸš«\"\n",
    "        print(f\"{status}{robots_status} {resultado.url[:50]}... - Status: {resultado.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š VisualizaÃ§Ãµes nÃ£o disponÃ­veis - instale pandas e matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Criar visualizaÃ§Ã£o (se pandas/matplotlib disponÃ­vel)\n",
    "if PANDAS_AVAILABLE:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # GrÃ¡fico 1: Permitido vs Bloqueado\n",
    "    permitidos = df['allowed'].value_counts()\n",
    "    if len(permitidos) == 2:\n",
    "        labels = ['Bloqueado', 'Permitido'] if False in permitidos.index else ['Permitido']\n",
    "    else:\n",
    "        labels = ['Permitido' if permitidos.index[0] else 'Bloqueado']\n",
    "    \n",
    "    ax1.pie(permitidos.values, labels=labels, autopct='%1.1f%%', \n",
    "            colors=['#ff6b6b', '#4ecdc4'] if len(permitidos) == 2 else ['#4ecdc4'])\n",
    "    ax1.set_title('URLs: Permitidas vs Bloqueadas')\n",
    "    \n",
    "    # GrÃ¡fico 2: Sites com/sem robots.txt\n",
    "    robots = df['robots_found'].value_counts()\n",
    "    if len(robots) == 2:\n",
    "        labels = ['Sem robots.txt', 'Com robots.txt'] if False in robots.index else ['Com robots.txt']\n",
    "    else:\n",
    "        labels = ['Com robots.txt' if robots.index[0] else 'Sem robots.txt']\n",
    "        \n",
    "    ax2.pie(robots.values, labels=labels, autopct='%1.1f%%',\n",
    "            colors=['#ffa726', '#66bb6a'] if len(robots) == 2 else ['#66bb6a'])\n",
    "    ax2.set_title('Sites com robots.txt')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"ğŸ“Š VisualizaÃ§Ãµes nÃ£o disponÃ­veis - instale pandas e matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“Š Step 5: Export Results to CSV and JSON\n\n**Now let's export the data to formats you can use in Excel, Python, or other tools.**\n\nğŸ“„ **Generated files**:\n- `scraping_results_YYYYMMDD_HHMMSS.csv` (Excel/Google Sheets)\n- `scraping_results_YYYYMMDD_HHMMSS.json` (programmatic analysis)\n\nğŸ“ **Destination folder**: Files will be saved in the current notebook folder"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export results\nfrom datetime import datetime\n\n# File name with timestamp\nfile_name = f\"scraping_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n\nif PANDAS_AVAILABLE:\n    # Export with pandas\n    df.to_csv(f\"{file_name}.csv\", index=False)\n    print(f\"ğŸ“„ Results saved to: {file_name}.csv\")\nelse:\n    # Export using BatchProcessor methods\n    batch_processor.export_to_csv(job_state, f\"{file_name}.csv\")\n    print(f\"ğŸ“„ Results saved to: {file_name}.csv\")\n\n# Export to JSON using BatchProcessor\nbatch_processor.export_to_json(job_state, f\"{file_name}.json\")\nprint(f\"ğŸ“„ Results saved to: {file_name}.json\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ›¡ï¸ Passo 6: PrincÃ­pios Ã‰ticos - O que VocÃª DEVE Saber\n\n### âœ… **O ScraperÃ‰tico SEMPRE faz automaticamente:**\n\n- ğŸ” **Verifica robots.txt** antes de cada acesso\n- â±ï¸ **Aplica delays** entre requests (nunca sobrecarrega)\n- ğŸ¤– **Identifica o bot** com user-agent claro\n- ğŸ“ **Gera logs completos** para auditoria\n- ğŸš¨ **Para se bloqueado** (erro 429, robots.txt)\n\n### âŒ **O ScraperÃ‰tico NUNCA faz:**\n\n- âŒ Ignora robots.txt ou termos de uso\n- âŒ Faz requests sem delay\n- âŒ Usa user-agents falsos de navegadores\n- âŒ Esconde a identidade do bot\n- âŒ Continua tentando quando bloqueado\n\n### ğŸš¨ **Suas responsabilidades como usuÃ¡rio:**\n\n1. **Configure user-agent** com SEU site e SEU email reais\n2. **Use delays adequados** (mÃ­nimo 1s, recomendado 3-5s para sites gov)\n3. **Monitore logs** regularmente\n4. **Respeite termos de uso** dos sites\n5. **Tenha propÃ³sito legÃ­timo** para o scraping\n\n### ğŸ“ **Exemplo de User-Agent Ã‰tico:**\n\n```python\n# âœ… BOM - Identifica claramente quem vocÃª Ã©\nuser_agent = \"MeuProjeto/1.0 (+https://github.com/usuario/projeto; contato@email.com)\"\nuser_agent = \"PesquisaTCC/1.0 (+https://universidade.br/tcc; aluno@univ.br)\"  \nuser_agent = \"AnalisePublica/1.0 (+https://empresa.com/pesquisa; pesquisa@empresa.com)\"\n\n# âŒ RUIM - GenÃ©rico demais\nuser_agent = \"MeuBot/1.0\"\nuser_agent = \"Python-requests/2.28\"  # PadrÃ£o do requests\n```\n\n### ğŸ“„ **IMPORTANTE: Configure no arquivo `config_producao.py`**\nPara produÃ§Ã£o, copie `config_producao.example.py` â†’ `config_producao.py` e edite:\n- `USER_AGENT` - Com seus dados reais\n- `SITES_PRODUCAO` - Com seus sites para monitorar  \n- `DEFAULT_DELAY` - Conforme tipo de sites (gov = 5s+)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:29 - ScraperEtico - INFO - ScraperEtico initialized with user-agent: MeuProjeto/2.0 (+http://meusite.com/sobre-bot)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Scraper personalizado configurado!\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de configuraÃ§Ã£o personalizada\n",
    "import logging\n",
    "\n",
    "scraper_personalizado = ScraperEtico(\n",
    "    user_agent=\"MeuProjeto/2.0 (+http://meusite.com/sobre-bot)\",\n",
    "    default_delay=2.0,  # Delay mais conservador\n",
    "    timeout=10.0,       # Timeout menor\n",
    "    log_level=logging.DEBUG  # Logs mais detalhados\n",
    ")\n",
    "\n",
    "# Acessar a sessÃ£o de requests para configurar headers\n",
    "import requests\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'User-Agent': scraper_personalizado.user_agent,\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',\n",
    "    'From': 'contato@meusite.com'  # Email para contato\n",
    "})\n",
    "\n",
    "# Usar a sessÃ£o customizada\n",
    "scraper_personalizado.session = session\n",
    "\n",
    "print(\"ğŸ”§ Scraper personalizado configurado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Casos de Uso PrÃ¡ticos\n",
    "\n",
    "### 1. VerificaÃ§Ã£o de Lista de Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:29 - ScraperEtico - DEBUG - Fetching robots.txt from: https://g1.globo.com/robots.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“° Verificando sites de notÃ­cias...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:29 - ScraperEtico - INFO - Successfully fetched robots.txt from https://g1.globo.com/robots.txt\n",
      "2025-09-04 12:58:29 - ScraperEtico - DEBUG - Robots.txt check for https://g1.globo.com/rss: allowed\n",
      "2025-09-04 12:58:29 - ScraperEtico - DEBUG - Fetching robots.txt from: https://folha.uol.com.br/robots.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Permitido - https://g1.globo.com/rss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:30 - ScraperEtico - INFO - Successfully fetched robots.txt from https://folha.uol.com.br/robots.txt\n",
      "2025-09-04 12:58:30 - ScraperEtico - DEBUG - Robots.txt check for https://folha.uol.com.br/rss: allowed\n",
      "2025-09-04 12:58:30 - ScraperEtico - DEBUG - Fetching robots.txt from: https://estadao.com.br/robots.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Permitido - https://folha.uol.com.br/rss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:30 - ScraperEtico - INFO - Successfully fetched robots.txt from https://estadao.com.br/robots.txt\n",
      "2025-09-04 12:58:30 - ScraperEtico - DEBUG - Robots.txt check for https://estadao.com.br/rss: allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Permitido - https://estadao.com.br/rss\n"
     ]
    }
   ],
   "source": [
    "# Lista de sites de notÃ­cias para verificar\n",
    "sites_noticias = [\n",
    "    \"https://g1.globo.com/rss\",\n",
    "    \"https://folha.uol.com.br/rss\",\n",
    "    \"https://estadao.com.br/rss\",\n",
    "]\n",
    "\n",
    "print(\"ğŸ“° Verificando sites de notÃ­cias...\")\n",
    "for site in sites_noticias:\n",
    "    try:\n",
    "        # API correta: can_fetch ao invÃ©s de verificar_robots\n",
    "        resultado = scraper.can_fetch(site)\n",
    "        status = \"âœ… Permitido\" if resultado else \"âŒ Bloqueado\"\n",
    "        print(f\"{status} - {site}\")\n",
    "        \n",
    "        # Verificar crawl-delay tambÃ©m\n",
    "        delay = scraper.get_crawl_delay(site)\n",
    "        if delay:\n",
    "            print(f\"   â±ï¸  Crawl-delay: {delay}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"â— Erro - {site}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. AnÃ¡lise de Diferentes User-Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:30 - ScraperEtico - INFO - ScraperEtico initialized with user-agent: *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Testando diferentes user-agents em https://example.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:31 - ScraperEtico - INFO - Successfully fetched robots.txt from https://example.com/robots.txt\n",
      "2025-09-04 12:58:31 - ScraperEtico - INFO - ScraperEtico initialized with user-agent: Googlebot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… *: Permitido\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:32 - ScraperEtico - INFO - Successfully fetched robots.txt from https://example.com/robots.txt\n",
      "2025-09-04 12:58:32 - ScraperEtico - INFO - ScraperEtico initialized with user-agent: Bingbot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Googlebot: Permitido\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:33 - ScraperEtico - INFO - Successfully fetched robots.txt from https://example.com/robots.txt\n",
      "2025-09-04 12:58:33 - ScraperEtico - INFO - ScraperEtico initialized with user-agent: MeuBot/1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Bingbot: Permitido\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 12:58:33 - ScraperEtico - INFO - Successfully fetched robots.txt from https://example.com/robots.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MeuBot/1.0: Permitido\n"
     ]
    }
   ],
   "source": [
    "# Testar diferentes user-agents no mesmo site\n",
    "site_teste = \"https://example.com\"\n",
    "user_agents = [\n",
    "    \"*\",  # Todos os bots\n",
    "    \"Googlebot\",\n",
    "    \"Bingbot\", \n",
    "    \"MeuBot/1.0\"\n",
    "]\n",
    "\n",
    "print(f\"ğŸ¤– Testando diferentes user-agents em {site_teste}\")\n",
    "for ua in user_agents:\n",
    "    scraper_temp = ScraperEtico(user_agent=ua)\n",
    "    resultado = scraper_temp.can_fetch(site_teste)\n",
    "    status = \"âœ…\" if resultado else \"âŒ\"\n",
    "    print(f\"{status} {ua}: {'Permitido' if resultado else 'Bloqueado'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“ Congratulations! You completed the tutorial\n\n### ğŸ‰ **What you learned:**\n\n- âœ… How to perform **ethical scraping** respecting robots.txt\n- âœ… How to process **multiple sites** in batches  \n- âœ… How to **export data** to CSV and JSON automatically\n- âœ… How to configure **delays and user-agents** properly\n- âœ… **Ethical principles** fundamental for web scraping\n\n### ğŸš€ **Next steps for production:**\n\n1. **ğŸ“„ Configure your real credentials in `production_config.py`:**\n   ```bash\n   cp production_config.example.py production_config.py\n   nano production_config.py  # OR use your preferred editor\n   ```\n\n2. **ğŸ§ª Test your specific sites by editing `tests/examples/my_monitoring.py`:**\n   ```bash\n   python3 tests/examples/my_monitoring.py\n   ```\n\n3. **ğŸš€ Execute production scraping with `run_production.py`:**\n   ```bash\n   python3 run_production.py\n   ```\n\n4. **ğŸ“Š Monitor and analyze results with `analyze_results.py`:**\n   ```bash\n   python3 analyze_results.py\n   open production_data/monitoring_*.csv\n   ```\n\n### ğŸ“š **Important project files:**\n\n- **ğŸ“– `README.md`** - Complete documentation\n- **ğŸ§ª `tests/production_test.py`** - Tests before production  \n- **ğŸ“Š `analyze_results.py`** - Automatic analysis\n- **ğŸ”§ `production_config.py`** - Your custom settings\n- **ğŸ“ `production_config.example.py`** - Configuration template\n- **ğŸ¯ `tests/examples/my_monitoring.py`** - Test with your specific sites\n- **âš¡ `run_production.py`** - Main production script\n- **ğŸ“‹ `production_checklist.txt`** - Checklist before production\n\n### ğŸ›¡ï¸ **Always remember:**\n\n> *\"With great power comes great responsibility\"*\n\n- **Always respect robots.txt** (never try to circumvent)\n- **Use adequate delays** (minimum 3s for gov sites)  \n- **Identify your bot clearly** (user-agent with your real data)\n- **Have legitimate purpose** (research, public monitoring)\n- **Monitor logs regularly** (`logs/` folder)\n\n### ğŸ†˜ **Need help?**\n\n- ğŸ“– **Read `README.md`** - Complete documentation with examples\n- ğŸ› **Problems?** Open an issue on GitHub\n- ğŸ’¬ **Questions?** Use GitHub Discussions\n\n**Now you're ready to perform ethical web scraping! ğŸ¤–âœ¨**\n\n### ğŸ’¡ **Final tip**: \nAlways start testing with **few sites** (3-5) before scaling to hundreds. EthicalScraper is robust, but being conservative is always better!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}