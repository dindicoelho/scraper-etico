from src.scraper_etico import ScraperEtico
from src.analyzer import RobotsAnalyzer
import time
from datetime import datetime

print("ğŸ’° MONITOR DE PREÃ‡OS PÃšBLICOS")
print("="*50)

# Seus sites especÃ­ficos (substitua pelos reais)
sites_licitacoes = [
    "https://www.comprasgovernamentais.gov.br",
    "https://www.gov.br/compras/pt-br",
    "https://www.bndes.gov.br/wps/portal/site/home/transparencia",
    # Adicione seus sites especÃ­ficos aqui
]

sites_economicos = [
    "https://www.bcb.gov.br/controleinflacao/historicotaxasjuros",
    "https://www.ibge.gov.br/estatisticas/economicas/precos-e-custos",
    "https://www.ipea.gov.br/portal/",
    # Adicione outros sites econÃ´micos
]

# ConfiguraÃ§Ã£o para sites governamentais (mais conservadora)
scraper = ScraperEtico(
    user_agent="MonitorPrecosPublicos/1.0 (+https://seusite.com/contato; seuemail@exemplo.com)",
    default_delay=5.0,  # 5 segundos - muito respeitoso
    timeout=20.0
)

analyzer = RobotsAnalyzer()

def analisar_site_detalhado(url, categoria):
    print(f"\nğŸ” ANALISANDO: {url}")
    print(f"ğŸ“‚ Categoria: {categoria}")
    
    try:
        # AnÃ¡lise completa do robots.txt
        analise = analyzer.analisar_site(url)
        
        if analise['robots_encontrado']:
            print(f"   ğŸ“‹ Robots.txt: âœ… Encontrado")
            print(f"   ğŸ¤– User-agents: {len(analise['user_agents'])}")
            print(f"   ğŸ“ Total regras: {analise['total_regras']}")
            
            if analise['crawl_delays']:
                for ua, delay in analise['crawl_delays'].items():
                    print(f"   â±ï¸  Delay para {ua}: {delay}s")
            
            if analise['sitemaps']:
                print(f"   ğŸ—ºï¸  Sitemaps encontrados: {len(analise['sitemaps'])}")
                for sitemap in analise['sitemaps'][:2]:  # Mostrar sÃ³ 2
                    print(f"      - {sitemap}")
        else:
            print(f"   ğŸ“‹ Robots.txt: âŒ NÃ£o encontrado")
        
        # Testar acesso especÃ­fico
        pode_acessar = scraper.can_fetch(url)
        
        if pode_acessar:
            print(f"   ğŸš¦ Acesso: âœ… PERMITIDO")
            
            # Tentar request real
            response = scraper.get(url)
            if response:
                print(f"   ğŸ“Š Request: âœ… Sucesso ({response.status_code})")
                print(f"   ğŸ“„ Tamanho: {len(response.text)} caracteres")
                
                # Verificar se tem dados Ãºteis
                if any(palavra in response.text.lower() for palavra in ['preÃ§o', 'valor', 'custo', 'licitaÃ§Ã£o']):
                    print(f"   ğŸ’° Dados Ãºteis: âœ… PossÃ­vel conteÃºdo relevante")
                else:
                    print(f"   ğŸ’° Dados Ãºteis: âš ï¸  ConteÃºdo nÃ£o identificado")
            else:
                print(f"   ğŸ“Š Request: âŒ Falhou")
        else:
            print(f"   ğŸš¦ Acesso: âŒ BLOQUEADO pelo robots.txt")
            
    except Exception as e:
        print(f"   ğŸ’¥ ERRO: {str(e)}")
    
    print(f"   â° Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Pausa respeitosa
    time.sleep(3)

# Executar anÃ¡lises
print(f"ğŸ¯ Iniciando anÃ¡lise de sites...")
print(f"âš ï¸  Usando delays de 5 segundos (muito conservador)")

print(f"\n" + "="*30)
print(f"ğŸ’¼ SITES DE LICITAÃ‡Ã•ES/COMPRAS")
print(f"="*30)

for site in sites_licitacoes[:2]:  # SÃ³ 2 para teste
    analisar_site_detalhado(site, "LicitaÃ§Ãµes")

print(f"\n" + "="*30)
print(f"ğŸ’¹ SITES ECONÃ”MICOS")
print(f"="*30)

for site in sites_economicos[:2]:  # SÃ³ 2 para teste
    analisar_site_detalhado(site, "EconÃ´micos")

print(f"\nğŸ‰ AnÃ¡lise concluÃ­da!")
print(f"\nğŸ’¡ PRÃ“XIMAS AÃ‡Ã•ES:")
print(f"   1. Foque nos sites que mostram âœ… PERMITIDO")
print(f"   2. Respeite os crawl-delays encontrados")
print(f"   3. Teste URLs especÃ­ficas dos sites permitidos")
print(f"   4. Configure monitoramento automÃ¡tico apenas para sites permitidos")
print(f"\nâš–ï¸  LEMBRE: Sempre verifique os termos de uso alÃ©m do robots.txt!")