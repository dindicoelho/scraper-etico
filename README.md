# ğŸ¤– ScraperÃ‰tico

Uma biblioteca Python completa para **web scraping Ã©tico** que respeita automaticamente robots.txt, implementa rate limiting e fornece anÃ¡lise avanÃ§ada - **perfeita para monitoramento de preÃ§os pÃºblicos**.

![Python](https://img.shields.io/badge/python-v3.8+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Status](https://img.shields.io/badge/status-Production%20Ready-brightgreen.svg)

## ğŸš€ ComeÃ§ando em 5 Minutos

### 1. Clone e Instale
```bash
git clone https://github.com/SEU-USUARIO/scraper-etico.git
cd scraper-etico
pip install -r requirements.txt
```

### 2. Configure suas credenciais
```bash
cp config_producao.example.py config_producao.py
nano config_producao.py  # Edite com seus dados reais
```

### 3. Teste bÃ¡sico
```bash
python3 example_usage.py
```

### 4. Execute scraping em produÃ§Ã£o
```bash
python3 rodar_producao.py
```

**Pronto! ğŸ‰** Seus dados vÃ£o aparecer em `dados_producao/`

## ğŸ’¡ Por que usar o ScraperÃ‰tico?

### âœ… **O que esta biblioteca faz POR VOCÃŠ:**
- ğŸ›¡ï¸ **Verifica robots.txt automaticamente** - Nunca vai quebrar regras
- â±ï¸ **Rate limiting inteligente** - Nunca vai sobrecarregar servidores
- ğŸ“Š **Export automÃ¡tico** para CSV e JSON - Dados prontos para anÃ¡lise
- ğŸ”„ **Processamento paralelo** - Processa centenas de sites rapidamente
- ğŸ“ **Logs completos** - Auditoria total do que aconteceu
- ğŸš¨ **Error handling robusto** - Continua funcionando mesmo com problemas

### âŒ **O que vocÃª NÃƒO precisa se preocupar:**
- âŒ Criar verificaÃ§Ã£o de robots.txt manual
- âŒ Implementar delays entre requests
- âŒ Tratar erros de rede e timeouts
- âŒ Converter dados para CSV/JSON
- âŒ Gerenciar logs e auditoria
- âŒ Configurar processamento paralelo

## ğŸ“‹ Casos de Uso Reais

### ğŸ›ï¸ **Monitoramento de LicitaÃ§Ãµes**
```python
sites_licitacoes = [
    "https://portal.compras.gov.br",
    "https://www.gov.br/compras/pt-br",
    "https://transparencia.gov.br"
]
# Resultado: CSV com status de cada site, dados extraÃ­dos, timestamps
```

### ğŸ’° **AnÃ¡lise de PreÃ§os Governamentais**
```python
sites_precos = [
    "https://www.bcb.gov.br/controleinflacao",
    "https://www.ibge.gov.br/estatisticas/economicas/precos-e-custos",
    "https://www.ipea.gov.br/portal/"
]
# Resultado: JSON com dados estruturados, mÃ©tricas, relatÃ³rios
```

## ğŸ“– Guia Completo de Uso

### ğŸ”° **NÃ­vel 1: Uso BÃ¡sico (1 site)**
```python
from src.scraper_etico import ScraperEtico

# Configurar com SEUS dados
scraper = ScraperEtico(
    user_agent="MeuProjeto/1.0 (+https://meusite.com; contato@email.com)",
    default_delay=3.0  # 3 segundos entre requests
)

# Verificar se pode acessar
if scraper.can_fetch("https://example.com"):
    response = scraper.get("https://example.com")
    print(f"Sucesso! Dados: {len(response.text)} caracteres")
else:
    print("Site bloqueou o acesso")
```

### ğŸ“Š **NÃ­vel 2: Processamento em Lote (mÃºltiplos sites)**
```python
from src.batch_processor import BatchProcessor

# Lista de sites para monitorar
urls = [
    "https://portal.compras.gov.br",
    "https://transparencia.gov.br",
    "https://www.gov.br/economia"
]

# Processar todos de uma vez
processor = BatchProcessor()
processor.scraper = ScraperEtico(
    user_agent="MeuBot/1.0 (+https://meusite.com; eu@email.com)",
    default_delay=5.0  # Respeitoso para sites gov
)

# Executar (vai demorar uns minutos)
job_state = processor.process_batch(
    urls,
    max_workers=2,      # 2 sites em paralelo
    show_progress=True  # Mostrar barra de progresso
)

# Export automÃ¡tico
processor.export_to_csv(job_state, "meus_resultados.csv")
processor.export_to_json(job_state, "meus_resultados.json")

print(f"âœ… Processados: {job_state.processed_count} sites")
print(f"ğŸ“Š Sucessos: {len(job_state.completed_urls)}")
```

### ğŸš€ **NÃ­vel 3: ProduÃ§Ã£o Automatizada (rodar todo dia)**

**1. Configure uma vez:**
```bash
# Editar configuraÃ§Ãµes
nano config_producao.py

# Adicionar seus sites
SITES_PRODUCAO = [
    "https://portal.compras.gov.br",
    "https://transparencia.gov.br",
    # Seus sites aqui...
]

# Configurar seu user-agent
USER_AGENT = "MeuProjeto/1.0 (+https://meusite.com; eu@email.com)"
```

**2. Execute diariamente:**
```bash
python3 rodar_producao.py
```

**3. Veja os resultados:**
```bash
# Arquivos gerados automaticamente
ls dados_producao/
# monitoramento_20241204_143022.csv
# monitoramento_20241204_143022.json

# Abrir CSV no Excel
open dados_producao/monitoramento_*.csv

# AnÃ¡lise automÃ¡tica
python3 analisar_resultados.py
```

## ğŸ“ **O que vocÃª recebe (Arquivos Gerados)**

### ğŸ“„ **Arquivo CSV** (Excel/Google Sheets)
```csv
url,domain,success,robots_allowed,status_code,response_size,timestamp
https://portal.compras.gov.br,portal.compras.gov.br,True,True,200,45621,2024-01-15T14:30:22
https://site-bloqueado.com,site-bloqueado.com,False,False,403,0,2024-01-15T14:30:27
```

### ğŸ“Š **Arquivo JSON** (AnÃ¡lise programÃ¡tica)
```json
{
  "job_metadata": {
    "total_urls": 10,
    "success_count": 8,
    "completion_percentage": 80.0,
    "duration": "00:03:45"
  },
  "results": [
    {
      "url": "https://portal.compras.gov.br",
      "success": true,
      "status_code": 200,
      "response_size": 45621,
      "response_time": 2.3,
      "robots_allowed": true,
      "timestamp": "2024-01-15T14:30:22"
    }
  ]
}
```

### ğŸ“‹ **Logs Detalhados** (`logs/producao_YYYYMMDD.log`)
```
2024-01-15 14:30:15 - ScraperEtico - INFO - Checking robots.txt for https://portal.compras.gov.br
2024-01-15 14:30:16 - ScraperEtico - INFO - Access allowed, applying 5.0s delay
2024-01-15 14:30:22 - ScraperEtico - INFO - Request successful: 200 OK (45621 bytes)
```

## âš™ï¸ **ConfiguraÃ§Ã£o Detalhada**

### ğŸ¤– **User-Agent (OBRIGATÃ“RIO configurar)**
```python
# âŒ ERRADO - GenÃ©rico demais
USER_AGENT = "MeuBot/1.0"

# âœ… CORRETO - IdentificaÃ§Ã£o completa
USER_AGENT = "MonitorPrecos/1.0 (+https://github.com/usuario/projeto; contato@email.com)"
USER_AGENT = "PesquisaAcademica/1.0 (+https://universidade.br/projeto; prof@univ.br)"
USER_AGENT = "AnaliseMercado/1.0 (+https://empresa.com/sobre-bot; dev@empresa.com)"
```

### â±ï¸ **Delays Recomendados**
```python
# Sites governamentais (seja muito respeitoso)
DEFAULT_DELAY = 5.0  # 5 segundos

# Sites comerciais
DEFAULT_DELAY = 2.0  # 2 segundos

# Sites pessoais/blogs
DEFAULT_DELAY = 1.0  # 1 segundo
```

### ğŸ§µ **Workers (ParalelizaÃ§Ã£o)**
```python
# Sites governamentais
MAX_WORKERS = 1  # Um de cada vez

# Sites comerciais
MAX_WORKERS = 2  # AtÃ© 2 paralelos

# Sites robustos (Google, GitHub, etc)
MAX_WORKERS = 3  # AtÃ© 3 paralelos
```

## ğŸ§ª **Testando Antes de Usar**

### **1. Teste seus sites especÃ­ficos**
```bash
# Editar lista de sites
nano teste_meus_sites.py

# Executar teste
python3 teste_meus_sites.py
```

### **2. Teste completo de produÃ§Ã£o**
```bash
python3 teste_producao.py
```

### **3. Tutorial interativo**
```bash
jupyter notebook notebooks/tutorial_scraper_etico.ipynb
```

## ğŸ“Š **AnÃ¡lise dos Resultados**

### **AnÃ¡lise automÃ¡tica**
```bash
python3 analisar_resultados.py
# Escolha opÃ§Ã£o 1: Analisar dados_producao/
```

**Resultado:**
```
ğŸ“Š RELATÃ“RIO COMPLETO DE SCRAPING
================================
ğŸ“ Pasta analisada: dados_producao
ğŸ“„ Arquivos CSV: 5
âœ… URLs com sucesso: 85 (78.7%)
âŒ URLs com falha: 23 (21.3%)
ğŸŒ TOP 5 DOMÃNIOS:
   portal.compras.gov.br: 15 URLs
   transparencia.gov.br: 12 URLs
   ...
```

### **Excel/Google Sheets**
```bash
# Abrir Ãºltimo arquivo
open dados_producao/monitoramento_*.csv

# Copiar para Ã¡rea de trabalho
cp dados_producao/monitoramento_*.csv ~/Desktop/
```

## ğŸ›¡ï¸ **PrincÃ­pios Ã‰ticos (Muito Importante!)**

### âœ… **Esta biblioteca SEMPRE faz:**
- Verifica robots.txt antes de QUALQUER request
- Aplica delays apropriados entre requests
- Identifica claramente o bot com user-agent descritivo
- Respeita crawl-delay especificado pelos sites
- Para imediatamente se receber erro 429 (Too Many Requests)
- Gera logs completos para auditoria

### âŒ **Esta biblioteca NUNCA faz:**
- Ignora robots.txt
- Faz requests simultÃ¢neos excessivos
- Usa user-agents falsos de navegadores
- Esconde a identidade do bot
- Continua tentando apÃ³s ser bloqueada

### ğŸš¨ **Responsabilidades do UsuÃ¡rio:**
- âœ… Configure user-agent com SEUS dados reais
- âœ… Use delays adequados (mÃ­nimo 1s, recomendado 3-5s)
- âœ… Monitore logs regularmente
- âœ… Respeite termos de uso dos sites
- âœ… Tenha um propÃ³sito legÃ­timo para o scraping

## ğŸ”§ **API ReferÃªncia RÃ¡pida**

### **ScraperEtico**
```python
scraper = ScraperEtico(
    user_agent="ObrigatÃ³rio: MeuBot/1.0 (+site; email)",
    default_delay=3.0,      # Segundos entre requests
    timeout=30.0            # Timeout por request
)

# MÃ©todos principais
scraper.can_fetch(url)      # bool: pode acessar?
scraper.get(url)           # requests.Response ou None
scraper.get_crawl_delay(url) # float: delay especÃ­fico do site
```

### **BatchProcessor**
```python
processor = BatchProcessor()
processor.scraper = ScraperEtico(...)

job_state = processor.process_batch(
    urls,                   # Lista de URLs
    max_workers=2,          # Threads paralelas
    show_progress=True      # Barra de progresso
)

# Export
processor.export_to_csv(job_state, "resultado.csv")
processor.export_to_json(job_state, "resultado.json")
```

## ğŸ› **Troubleshooting**

### **"ModuleNotFoundError: No module named 'src'"**
```bash
# Certifique-se de estar na pasta correta
cd scraper_etico
python3 -c "import sys; print(sys.path[0])"  # Deve mostrar a pasta atual
```

### **"Access disallowed by robots.txt"**
```
âœ… Comportamento normal! O site nÃ£o permite bots.
âŒ NÃƒO tente contornar - respeite a decisÃ£o do site
ğŸ’¡ Considere usar API oficial se disponÃ­vel
```

### **"Too many requests (429)"**
```
âœ… Biblioteca para automaticamente
ğŸ’¡ Aumente DEFAULT_DELAY para 5-10 segundos
ğŸ’¡ Reduza MAX_WORKERS para 1
```

### **Arquivos CSV nÃ£o abrem no Excel**
```bash
# Converta encoding se necessÃ¡rio
iconv -f UTF-8 -t ISO-8859-1 dados_producao/arquivo.csv > arquivo_excel.csv
```

## ğŸ“¦ **DependÃªncias**

### **ObrigatÃ³rias** (jÃ¡ no requirements.txt)
- `requests >= 2.28.0` - HTTP requests
- Python 3.8+ (built-in urllib para robots.txt)

### **Opcionais** (para recursos avanÃ§ados)
```bash
# Para tutorial Jupyter
pip install jupyter pandas matplotlib

# Para testes
pip install pytest pytest-cov

# Para barras de progresso
pip install tqdm
```

## ğŸ¯ **Casos de Uso EspecÃ­ficos**

### **Monitoramento DiÃ¡rio de LicitaÃ§Ãµes**
```python
# config_producao.py
SITES_PRODUCAO = [
    "https://portal.compras.gov.br",
    "https://www.licitacoes-e.com.br", 
    "https://transparencia.gov.br"
]
DEFAULT_DELAY = 10.0  # Muito respeitoso
MAX_WORKERS = 1       # Um de cada vez
```

### **Pesquisa AcadÃªmica**
```python
USER_AGENT = "PesquisaTCC/1.0 (+https://universidade.br/projeto-tcc; aluno@univ.br)"
DEFAULT_DELAY = 3.0
# Documente tudo para orientador
```

### **AnÃ¡lise de Mercado**
```python
USER_AGENT = "EstudoMercado/1.0 (+https://empresa.com/pesquisa; pesquisa@empresa.com)"
# Foque em sites que permitem scraping
# Considere APIs oficiais quando possÃ­vel
```

## ğŸš€ **Rotina de ProduÃ§Ã£o Recomendada**

### **Setup inicial (uma vez sÃ³):**
```bash
git clone [repositÃ³rio]
cd scraper_etico
pip install -r requirements.txt
cp config_producao.example.py config_producao.py
nano config_producao.py  # Configurar
python3 teste_meus_sites.py  # Testar
```

### **ExecuÃ§Ã£o diÃ¡ria:**
```bash
# 8h da manhÃ£
python3 rodar_producao.py

# 9h da manhÃ£ - anÃ¡lise
python3 analisar_resultados.py
open dados_producao/monitoramento_*.csv
```

### **ManutenÃ§Ã£o semanal:**
```bash
# Backup
cp -r dados_producao/ backup_$(date +%Y%m%d)/

# Limpeza de logs antigos
find logs/ -name "*.log" -mtime +7 -delete

# Verificar espaÃ§o em disco
df -h
```

## ğŸ“„ **LicenÃ§a**

MIT License - Use livremente, mas mantenha os crÃ©ditos.

## ğŸ†˜ **Suporte**

- ğŸ“– **DocumentaÃ§Ã£o**: Este README + notebook tutorial
- ğŸ› **Bugs**: Abra issue no GitHub
- ğŸ’¬ **DÃºvidas**: GitHub Discussions
- ğŸ“§ **Contato**: Issues do GitHub

## âš ï¸ **Disclaimer Legal**

Esta ferramenta Ã© para **uso Ã©tico e educacional**. UsuÃ¡rios sÃ£o responsÃ¡veis por:
- âœ… Respeitar robots.txt e termos de uso
- âœ… Verificar legalidade do scraping em sua jurisdiÃ§Ã£o
- âœ… NÃ£o sobrecarregar servidores
- âœ… Ter propÃ³sito legÃ­timo para coleta de dados

---

## ğŸŒŸ **Scraping Ã‰tico Ã© Scraping ResponsÃ¡vel** 

*"Com grandes poderes vÃªm grandes responsabilidades"* 

**Made with â¤ï¸ for ethical web scraping**